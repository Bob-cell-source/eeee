"""
DeepResearch æœåŠ¡ - å¤šè½®è¿­ä»£ç ”ç©¶ (IterResearch)

è¯¥æœåŠ¡é‡‡ç”¨ IterResearch æ–¹æ³•ï¼š
1. æ¯è½®ç ”ç©¶éƒ½ä¼šæ ¹æ®æ ¸å¿ƒæŠ¥å‘Šå’Œæ£€ç´¢åˆ°çš„è¯æ®æ„å»ºä¸€ä¸ªæœ€å°çš„â€œå·¥ä½œç©ºé—´â€ã€‚
2. ActionDecider æ¨¡å—å†³å®šæ˜¯ç»§ç»­æœç´¢è¿˜æ˜¯è¾“å‡ºæœ€ç»ˆç­”æ¡ˆã€‚
3. è¯æ®å­˜å‚¨åœ¨ Milvus æ•°æ®åº“ä¸­ï¼Œè€Œä¸æ˜¯ç›´æ¥ä¼ é€’åˆ°ä¸Šä¸‹æ–‡ã€‚
4. æ ¸å¿ƒæŠ¥å‘Šä¼šéšç€è½®æ¬¡è¿­ä»£è€Œä¸æ–­å®Œå–„ã€‚
5. å½“è¾¾åˆ°æœ€å¤§è½®æ•°æˆ–æ‰€æœ‰å¾…è§£å†³é—®é¢˜éƒ½å·²è§£å†³æ—¶ï¼Œç ”ç©¶åœæ­¢ã€‚
"""
import logging
import json
import uuid
from typing import Dict, Any, Optional, List, Annotated, TypedDict, Literal,cast,AsyncIterator
from datetime import datetime
from enum import Enum
import os
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.graph.message import add_messages
from pydantic import SecretStr
from langgraph.checkpoint.memory import MemorySaver
import operator
from langchain_core.runnables import RunnableConfig

from ..tools.langchain_tool import TOOLS
from ..tools.embeddings import get_embedding_provider
from ..memory.writer import get_writer
from ..memory.retriever import get_retriever
from ..utils.hashing import generate_evidence_id, hash_text
from ..utils.tracing import (
    get_tracing_config,
    trace_node,
    trace_llm_call,
    print_workflow_diagram,
)
from ..utils.chunking import get_default_chunker, DocChunk
from .session_cache import SessionCache, RawDoc, ResearchNote, EvidenceQuote
from dotenv import load_dotenv
load_dotenv()
import os
logger = logging.getLogger(__name__)
from ..config import get_settings    
settings = get_settings()
DASHSCOPE_BASE_URL = os.getenv("LLM_BASE_URL") or "https://dashscope.aliyuncs.com/compatible-mode/v1"
import random


def make_llm(model: str, temperature: float = 0) -> ChatOpenAI:
    """æ„é€ å¤§æ¨¡å‹è°ƒç”¨"""
    key = os.getenv("LLM_API_KEY")
    if not key:
        raise RuntimeError("Missing DASHSCOPE_API_KEY")
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        base_url=DASHSCOPE_BASE_URL,
        api_key=SecretStr(key),
    )


def message_to_text(content: Any) -> str:
    """æ‹¼å‡ºçº¯æ–‡æœ¬å­—ç¬¦ä¸²"""
    if isinstance(content, str):
        return content
    # LangChain æœ‰æ—¶æ˜¯ list[ str | dict ]
    if isinstance(content, list):
        parts = []
        for p in content:
            if isinstance(p, str):
                parts.append(p)
            elif isinstance(p, dict):
                # å¸¸è§æ˜¯ {"type": "text", "text": "..."} ä¹‹ç±»
                parts.append(str(p.get("text", "")))
        return "".join(parts)
    return str(content)

def _extract_text_from_response(response_obj) -> str:
    """å°†å„ç§å“åº”æ ¼å¼è§„èŒƒåŒ–ä¸ºçº¯æ–‡æœ¬å­—ç¬¦ä¸²ã€‚ 

    æ”¯æŒçš„ç±»å‹åŒ…æ‹¬ï¼šå­—ç¬¦ä¸²ã€å­—èŠ‚ä¸²ã€åˆ—è¡¨ï¼ˆåŒ…å«å­—ç¬¦ä¸²æˆ–ç±»ä¼¼æ¶ˆæ¯çš„å¯¹è±¡ï¼‰ã€å­—å…¸
    ä»¥åŠå…·æœ‰ `.content` å±æ€§çš„å¯¹è±¡ã€‚ 
    åŒæ—¶ä¼šå»é™¤ä¸‰é‡åå¼•å·ä»£ç å—ä»¥åŠå¯é€‰çš„å¼€å¤´â€œjsonâ€æ ‡è¯†ç¬¦ã€‚.
    """
    raw = getattr(response_obj, "content", response_obj)

    # list of things (messages / strings)
    if isinstance(raw, list):
        text = None
        for item in raw:
            if isinstance(item, str):
                text = item
                break
            if hasattr(item, "content") and isinstance(item.content, str):
                text = item.content
                break
        if text is None:
            try:
                text = json.dumps(raw)
            except Exception:
                text = str(raw)

    elif isinstance(raw, dict):
        # common keys that might contain textual content
        for k in ("content", "text", "message", "result"):
            if k in raw and isinstance(raw[k], str):
                text = raw[k]
                break
        else:
            try:
                text = json.dumps(raw)
            except Exception:
                text = str(raw)

    else:
        text = raw if isinstance(raw, str) else str(raw)

    text = text.strip()
    if text.startswith("```"):
        parts = text.split("```")
        if len(parts) >= 2:
            block = parts[1]
            if block.lower().startswith("json"):
                block = block[4:].lstrip()
            text = block.strip()

    return text

class NodeCompleteEvent(TypedDict):
    type: Literal["node_complete"]
    node: str
    state: Dict[str, Any]   # âœ… node è¾“å‡º patch
    task_id: str
# ============================================================================
# EvidencePack data structure and helpers
# ============================================================================

class EvidencePack(TypedDict, total=False):
    """
    ç»“æ„åŒ–è¯æ®ç¬”è®°ï¼ˆå·²å¼±åŒ–ï¼Œæ¨èä½¿ç”¨ ResearchNoteï¼‰
    
    ä¿ç•™å…¼å®¹æ€§ï¼Œä½†æ–°ä»£ç åº”ä½¿ç”¨ session_cache.ResearchNote
    """
    pack_id: str
    source: str  # "arxiv"|"github"|"web"|"local_file"
    ref: str  # arxiv_id / repo full_name / url / file_path
    url: Optional[str]
    title: Optional[str]
    snippet: str  # <= 300 charsï¼ˆæ€»ä½“æ‘˜è¦ï¼‰
    key_points: List[str]  # æ¥è‡ª content_extract
    evidence_quotes: List[Dict[str, Any]]  # æ¥è‡ª content_extract
    raw_pointer: Optional[str]  # doc_idï¼Œç”¨äºæŒ‰éœ€å–åŸæ–‡
    confidence: str  # "high"|"medium"|"low"
    relevance: str  # one sentence
    # å¤šæ¨¡æ€
    media_type: str  # "text"|"image"
    image_url: Optional[str]
    # å…ƒä¿¡æ¯
    fetched_at: Optional[str]  # ISO timestamp
    target_question: Optional[str]  # å¯¹åº”çš„ open_question


def _make_pack_from_item(item: Dict[str, Any], tool: str) -> EvidencePack:
    """
    [DEPRECATED] æŠŠå¤–éƒ¨å·¥å…·è¿”å›çš„åŸå§‹ç»“æœ item è½¬æ¢æˆä¸€ä¸ªç´§å‡‘çš„ EvidencePack
    
    âš ï¸ è¯¥å‡½æ•°ä»…ç”¨äºç”Ÿæˆ"çº¿ç´¢ pack"ï¼ˆlead packï¼‰ï¼ŒçœŸæ­£çš„ EvidencePack åº”è¯¥ç”±
    content_extract ä» doc.text ç”Ÿæˆã€‚
    
    æ–°ä»£ç åº”è¯¥ä½¿ç”¨ materialize_round_cache_node ä¸­çš„ hitâ†’docâ†’note æµç¨‹ã€‚
    """
    # Identify source and extract fields
    if "arxiv" in tool.lower() or "arxiv_id" in item:
        source = "arxiv"
        ref = item.get("arxiv_id") or item.get("id", "")
        title = item.get("title", "")
        snippet = (item.get("summary") or "")[:300]
        url = item.get("abs_url") or item.get("url")
    elif "github" in tool.lower() or "full_name" in item:
        source = "github"
        ref = item.get("full_name") or item.get("name", "")
        title = item.get("name") or item.get("full_name")
        snippet = (item.get("description") or "")[:300]
        url = item.get("url")
    else:
        source = tool or "web"
        ref = item.get("url") or item.get("id", "")
        title = item.get("title") or item.get("name", "")
        snippet = (item.get("snippet") or item.get("content") or "")[:300]
        url = item.get("url")
    
    # Extract key points (simple heuristic: split into sentences, take first 5)
    raw_text = (item.get("summary") or item.get("description") or snippet)[:1000]
    sentences = [s.strip() for s in raw_text.split(". ") if s.strip()]
    key_points = [(s[:120] + ("..." if len(s) > 120 else "")) for s in sentences[:5]]
    
    pack_id = generate_evidence_id(source, ref or title or url or "", hash_text(snippet))
    
    pack: EvidencePack = {
        "pack_id": pack_id,
        "source": source,
        "ref": ref,
        "url": url,
        "title": title,
        "snippet": snippet,
        "key_points": key_points,
        "relevance": "",
        "confidence": "medium",
        "raw_pointer": None,
    }
    return pack


# ============================================================================
# State Definition (TypedDict)
# ============================================================================

class ResearchState(TypedDict):
    """
    LangGraph State for DeepResearch workflow
    
    çŠ¶æ€å­—æ®µä¼šåœ¨èŠ‚ç‚¹ä¹‹é—´ä¼ é€’ï¼ŒèŠ‚ç‚¹å¯ä»¥è¯»å–å’Œæ›´æ–°çŠ¶æ€ã€‚
    Annotated ç”¨äºå®šä¹‰çŠ¶æ€æ›´æ–°ç­–ç•¥ï¼ˆadd/replaceï¼‰ã€‚
    """
    # æ ¸å¿ƒå­—æ®µ
    task_id: str
    query: str
    round_id: int

    # ä»»åŠ¡æè¿°
    task_spec: Dict[str, Any]

    # ä¸­å¿ƒæŠ¥å‘Š
    summary: str
    findings: Annotated[List[str], operator.add]  # appendæ¨¡å¼
    open_questions: List[str]  # replaceæ¨¡å¼
    evidence_ids: Annotated[List[str], operator.add]

    # === Session Cacheï¼ˆæ–°å¢ï¼‰===
    session_cache: Dict[str, Any]  # SessionCache.to_state_dict() åºåˆ—åŒ–ç»“æœ
    round_doc_ids: List[str]  # æœ¬è½®æ–°å¢ doc_idï¼ˆreplaceï¼‰
    all_notes: Annotated[List[ResearchNote], operator.add]  # ç´¯è®¡è¯æ®ç¬”è®°
    round_notes: List[ResearchNote]  # æœ¬è½®è¯æ®ç¬”è®°ï¼ˆreplaceï¼‰
    persist_queue: Annotated[List[str], operator.add]  # å¾…æŒä¹…åŒ–çš„ pack_id/doc_id
    
    # Evidence packs (ä¿ç•™å…¼å®¹ï¼Œä½†æ¨èä½¿ç”¨ all_notes/round_notes)
    evidence_packs: Annotated[List[EvidencePack], operator.add]  # new packs per round
    new_evidence_packs: List[EvidencePack]   # replace

    # Workspace ä¸Šä¸‹æ–‡
    workspace: str

    # æ‰§è¡Œå·¥å…·
    tool_queries: List[str]
    tool_results: List[Dict[str, Any]]

    # å·¥ä½œæµæ§åˆ¶å’Œå†³å®š
    next_action: Literal["continue", "answer", "stop"]
    stop_reason: Optional[str]

    # è¿”å›ç»™LLMçš„message
    messages: Annotated[List[Any], add_messages]

    # output
    final_answer: str
    citations: List[Dict[str, Any]]

    # configuration
    max_rounds: int
    max_papers: int
    max_repos: int
    top_k: int

    # Tool loop / workspace limits (for progressive disclosure)
    max_tool_steps_per_round: int  # default 3-5
    max_workspace_chars: int  # default 6000-10000
    max_packs_per_step: int  # default 3-5
    max_total_packs_per_round: int  # default 10-20
    
    # Trace for UI
    trace: Annotated[List[Dict[str, Any]], operator.add]

class ResearchStateUpdate(TypedDict, total=False):
    task_spec: Dict[str, Any]
    open_questions: List[str]
    trace: List[Dict[str, Any]]
    workspace: str
    next_action: Literal["continue", "answer", "stop"]
    stop_reason: Optional[str]
    tool_queries: List[str]
    tool_results: List[Dict[str, Any]]
    evidence_ids: List[str]
    summary: str
    findings: List[str]
    final_answer: str
    citations: List[Dict[str, Any]]
    round_id: int
    messages: List[Any]
    evidence_packs: List[EvidencePack]  # new packs per round
    new_evidence_packs: List[EvidencePack]   # replace
    # Session Cache
    session_cache: Dict[str, Any]
    round_doc_ids: List[str]
    all_notes: List[ResearchNote]
    round_notes: List[ResearchNote]
    persist_queue: List[str]

# ============================================================================
# Graph Nodes (ä¸šåŠ¡é€»è¾‘)
# ============================================================================

@trace_node("parse_task")
async def parse_task_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: è§£æç”¨æˆ·æŸ¥è¯¢ï¼Œæå–ä»»åŠ¡è§„æ ¼
    
    è¾“å…¥: query
    è¾“å‡º: task_spec, open_questions
    """
    logger.info(f"[Task {state['task_id']}] Parsing task spec")
    logger.debug(f"ğŸ“¥ è¾“å…¥ query: {state['query'][:100]}...")
    
    llm = make_llm(model="qwen3-max")
    
    # åŠ¨æ€è·å–å½“å‰æ—¥æœŸä¿¡æ¯
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_year = datetime.now().year
    current_month = datetime.now().month
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""ä½ æ˜¯ä¸€ä¸ªç ”ç©¶ä»»åŠ¡è§„åˆ’åŠ©æ‰‹ã€‚å½“å‰æ—¥æœŸï¼š{current_date}ï¼ˆ{current_year}å¹´{current_month}æœˆï¼‰ã€‚

ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æ™ºèƒ½åˆ†æå¹¶æå–ç ”ç©¶ä»»åŠ¡è§„æ ¼ï¼Œè¿”å› JSON å¯¹è±¡åŒ…å«ï¼š
- topic: ä¸»è¦ç ”ç©¶ä¸»é¢˜ï¼ˆä¸­æ–‡ï¼‰
- specific_questions: å…·ä½“è¦å›ç­”çš„é—®é¢˜åˆ—è¡¨ï¼ˆä¸­æ–‡ï¼‰
  * å¦‚æœåŸé—®é¢˜å·²ç»è¶³å¤Ÿå…·ä½“ï¼Œä¿æŒä¸ºå•ä¸ªé—®é¢˜
  * å¦‚æœåŸé—®é¢˜è¾ƒå®½æ³›ï¼Œåˆ†è§£ä¸º2-5ä¸ªå¯ç‹¬ç«‹å›ç­”çš„å­é—®é¢˜
  * åˆ†è§£æ—¶ç¡®ä¿å­é—®é¢˜äº’ä¸é‡å ã€è¦†ç›–åŸé—®é¢˜çš„æ ¸å¿ƒç»´åº¦
- output_type: æœŸæœ›çš„è¾“å‡ºæ ¼å¼ï¼ˆsummary/comparison/analysis/list/reportç­‰ï¼‰
- time_range: æ—¶é—´èŒƒå›´å¯¹è±¡ï¼ˆåŸºäºqueryè¯­ä¹‰æ™ºèƒ½æ¨æ–­ï¼‰ï¼ŒåŒ…å«ï¼š
  * description: æ—¶é—´æè¿°ï¼ˆå¦‚"è¿‡å»äº”å¹´"ã€"2023-2024"ã€"ä¸é™"ï¼‰
  * start_year: èµ·å§‹å¹´ä»½ï¼ˆæ•°å­—æˆ–nullï¼‰
  * end_year: ç»“æŸå¹´ä»½ï¼ˆæ•°å­—æˆ–nullï¼Œé»˜è®¤å½“å‰å¹´ä»½{current_year}ï¼‰
  * priority: æ—¶æ•ˆæ€§è¦æ±‚ï¼ˆ"latest"=ä¼˜å…ˆæœ€æ–°/"all"=ä¸é™æ—¶é—´/"specific"=ç‰¹å®šæ—¶é—´æ®µï¼‰
- constraints: æåˆ°çš„ä»»ä½•çº¦æŸæˆ–åå¥½ï¼ˆä¸­æ–‡åˆ—è¡¨ï¼‰

**æ—¶é—´èŒƒå›´æ¨æ–­æŒ‡å—**ï¼ˆæ ¹æ®queryè‡ªè¡Œåˆ¤æ–­ï¼‰ï¼š
- "æœ€è¿‘"ã€"è¿‘æœŸ"ã€"å½“å‰" â†’ æœ€è¿‘1-3ä¸ªæœˆï¼Œpriority="latest"
- "è¿‡å»Nå¹´" â†’ start_year={current_year}-N, end_year={current_year}
- "YYYY-YYYY" â†’ æå–å…·ä½“å¹´ä»½èŒƒå›´
- "è¿›å±•"ã€"è¶‹åŠ¿"ã€"å‘å±•" â†’ æš—ç¤ºéœ€è¦æ—¶é—´è·¨åº¦ï¼Œé»˜è®¤è¿‡å»3-5å¹´
- æ— æ—¶é—´è¯ â†’ start_year=null, end_year=null, priority="all"

**ä»»åŠ¡åˆ†è§£æŒ‡å—**ï¼š
- ç®€å•æŸ¥è¯¢ï¼ˆå¦‚"ä»€ä¹ˆæ˜¯RAG"ï¼‰ â†’ ä¸åˆ†è§£ï¼Œä¿æŒ1ä¸ªé—®é¢˜
- ç»¼è¿°å‹æŸ¥è¯¢ï¼ˆå¦‚"æ¨èç³»ç»Ÿè¿›å±•"ï¼‰ â†’ åˆ†è§£ä¸ºæ–¹æ³•/åº”ç”¨/æŒ‘æˆ˜ç­‰2-4ä¸ªç»´åº¦
- å¯¹æ¯”å‹æŸ¥è¯¢ï¼ˆå¦‚"A vs B"ï¼‰ â†’ åˆ†è§£ä¸ºå„è‡ªç‰¹ç‚¹+å·®å¼‚å¯¹æ¯”

åªè¿”å›æœ‰æ•ˆçš„ JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""),
        ("human", "{query}"),
    ])
    
    
    chain = prompt | llm
    response = await chain.ainvoke({"query": state["query"]})
    
    import json
    try:
        
        raw = message_to_text(response.content)
        content = raw.strip()

        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
        task_spec = json.loads(content)
    except:
        task_spec = {
            "topic": state["query"],
            "specific_questions": [state["query"]],
            "output_type": "summary",
            "time_range": {
                "description": "ä¸é™",
                "start_year": None,
                "end_year": None,
                "priority": "all"
            },
            "constraints": [],
        }
    
    return {
        "task_spec": task_spec,
        "open_questions": task_spec.get("specific_questions", [state["query"]]),
        "trace": [{
            "round": 0,
            "stage": "parse_task",
            "timestamp": datetime.now().isoformat(),
            "message": "Task specification parsed",
            "data": task_spec,
        }],
    }

@trace_node("build_workspace")
async def build_workspace_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: æ„å»ºå·¥ä½œç©ºé—´ï¼ˆé‡æ„ç‰ˆï¼šmulti-query recall + notesä¼˜å…ˆï¼‰
    
    ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä½¿ç”¨ all_notesï¼ˆæœ¬æ¬¡ä¼šè¯ç§¯ç´¯ï¼‰
    2. å¤šæŸ¥è¯¢å¬å›ï¼šquery + open_questions[:3] + summary[:300]
    3. å»é‡åˆå¹¶ï¼Œå±•ç¤ºç´§å‡‘çš„ workspace
    
    è¾“å…¥: query, summary, findings, open_questions, all_notes, round_id
    è¾“å‡º: workspace
    """
    logger.info(f"[Task {state['task_id']}] Building workspace for round {state['round_id']}")
    logger.debug(f"ğŸ“¥ All notes: {len(state.get('all_notes', []))}, Findings: {len(state.get('findings', []))}")
    
    from ..services.session_cache import safe_note_pack_id
    
    embedding_provider = get_embedding_provider()
    retriever = get_retriever()
    
    # === Multi-query recall ===
    queries_to_embed = [state["query"]]
    
    # æ·»åŠ å‰ 3 ä¸ªå¾…è§£å†³é—®é¢˜
    open_questions = state.get("open_questions", [])
    for q in open_questions[:3]:
        if len(q) > 10:  # è¿‡æ»¤å¤ªçŸ­çš„é—®é¢˜
            queries_to_embed.append(q)
    
    # æ·»åŠ æ‘˜è¦å‰ 300 å­—
    summary = state.get("summary", "")
    if summary and len(summary) > 50:
        queries_to_embed.append(summary[:300])
    
    logger.debug(f"Multi-query recall with {len(queries_to_embed)} queries")
    
    # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œå¬å›
    all_recalled_ids = set()
    recalled_evidence_list = []
    max_per_query = max(5, state["top_k"] // len(queries_to_embed))
    
    for query_text in queries_to_embed:
        try:
            query_embedding = await embedding_provider.embed(query_text)
            recalled = retriever.search_evidence(
                query_embedding=query_embedding,
                top_k=max_per_query,
                task_id=state["task_id"],
            )
            for ev in recalled:
                ev_id = ev.get("id", "")
                if ev_id and ev_id not in all_recalled_ids:
                    all_recalled_ids.add(ev_id)
                    recalled_evidence_list.append(ev)
        except Exception as e:
            logger.warning(f"Recall failed for query '{query_text[:50]}': {e}")
            continue
    
    logger.info(f"Multi-query recalled {len(recalled_evidence_list)} unique evidence items")
    
    # è½¬æ¢ recalled evidence ä¸º EvidencePackï¼ˆä»…ç”¨äºå‘åå…¼å®¹ï¼‰
    recalled_packs: List[EvidencePack] = []
    max_packs = state.get("max_total_packs_per_round", 20)
    for ev in recalled_evidence_list[:max_packs]:
        pack: EvidencePack = {
            "pack_id": ev.get("id", ""),
            "source": ev.get("source", ""),
            "ref": ev.get("ref", ""),
            "url": ev.get("metadata", {}).get("url"),
            "title": ev.get("metadata", {}).get("title"),
            "snippet": (ev.get("snippet") or "")[:300],
            "key_points": [],
            "relevance": "",
            "confidence": "medium",
            "raw_pointer": None,
        }
        recalled_packs.append(pack)

    # === ä¼˜å…ˆä½¿ç”¨ all_notesï¼ˆæœ¬è½®ä¼šè¯ç§¯ç´¯ï¼‰===
    all_notes = state.get("all_notes", [])
    
    # Merge notes + recalled packs (notes ä¼˜å…ˆ)
    seen_ids = set()
    unique_items: List[Any] = []
    
    # å…ˆæ·»åŠ  all_notes
    for note in all_notes:
        note_id = safe_note_pack_id(note)
        if note_id not in seen_ids:
            seen_ids.add(note_id)
            unique_items.append(("note", note))
    
    # å†æ·»åŠ  recalled packsï¼ˆå»é‡ï¼‰
    for pack in recalled_packs:
        pack_id = pack.get("pack_id", "")
        if pack_id and pack_id not in seen_ids:
            seen_ids.add(pack_id)
            unique_items.append(("pack", pack))

    
    # === æ„å»º workspaceï¼ˆç´§å‡‘å±•ç¤ºï¼‰===
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    workspace_parts = [
        f"## ç ”ç©¶é—®é¢˜\n{state['query']}",
        f"\n## å½“å‰æ—¶é—´\n{current_date}",
        f"\n## å½“å‰æ€»ç»“\n{state.get('summary') or 'æš‚æ— æ€»ç»“ã€‚'}",
    ]
    
    # ä»»åŠ¡è§„æ ¼ï¼ˆæˆªæ–­ï¼‰
    try:
        ts = state.get("task_spec", {}) or {}
        task_spec_json = json.dumps(ts, ensure_ascii=False)
    except Exception:
        task_spec_json = str(state.get("task_spec", {}))
    if task_spec_json:
        task_spec_snippet = task_spec_json if len(task_spec_json) <= 1000 else task_spec_json[:1000] + " ... (å·²æˆªæ–­)"
        workspace_parts.append(f"\n## ä»»åŠ¡è§„æ ¼ï¼ˆå·²æˆªæ–­ï¼‰\n{task_spec_snippet}")
    
    if state.get("open_questions"):
        workspace_parts.append(
            f"\n## å¾…è§£å†³é—®é¢˜\n" + "\n".join(f"- {q}" for q in state["open_questions"])
        )
    
    if state.get("findings"):
        workspace_parts.append(
            f"\n## æœ€è¿‘å‘ç°\n" + "\n".join(f"- {f}" for f in state["findings"][-5:])
        )
    
    # è¯æ®å±•ç¤ºï¼ˆnotes ä¼˜å…ˆï¼Œç´§å‡‘æ ¼å¼ï¼‰
    if unique_items:
        evidence_summary = "\n## è¯æ®ï¼ˆç´§å‡‘ï¼‰\n"
        from ..services.session_cache import safe_note_evidence_quotes
        
        for item_type, item in unique_items[:max_packs]:
            if item_type == "note":
                # ResearchNoteï¼šæ˜¾ç¤º key_points + evidence_quotes
                pack_id = safe_note_pack_id(item)
                source = item.get("source", "")
                ref = item.get("ref", "")
                snippet = item.get("snippet", "")
                key_points = item.get("key_points", [])[:3]
                media_type = item.get("media_type", "text")
                
                if media_type == 'image':
                    image_url = item.get('image_url', '')
                    evidence_summary += f"- [IMAGE] [{pack_id}] {snippet[:120]}\n"
                    evidence_summary += f"  ğŸ–¼ï¸ URL: {image_url}\n"
                else:
                    evidence_summary += f"- [{pack_id}] {source}:{ref} - {snippet[:120]}\n"
                    for kp in key_points:
                        evidence_summary += f"  * {kp}\n"
            
            elif item_type == "pack":
                # EvidencePackï¼ˆå‘åå…¼å®¹ï¼‰
                p = item
                media_type = p.get('media_type', 'text')
                
                if media_type == 'image':
                    image_url = p.get('image_url', '')
                    evidence_summary += f"- [IMAGE] [{p.get('pack_id', '')}] {p.get('snippet', '')[:120]}\n"
                    evidence_summary += f"  ğŸ–¼ï¸ URL: {image_url}\n"
                else:
                    evidence_summary += f"- [{p.get('pack_id', '')}] {p.get('source', '')}:{p.get('ref', '')} - {p.get('snippet', '')[:120]}\n"
                
                for kp in p.get("key_points", [])[:3]:
                    evidence_summary += f"  * {kp}\n"
        
        workspace_parts.append(evidence_summary)
    
    workspace = "\n".join(workspace_parts)
    
    # Truncate to max_workspace_chars if needed
    max_chars = state.get("max_workspace_chars", 8000)
    if len(workspace) > max_chars:
        workspace = workspace[:max_chars] + "\n... (truncated)"
    
    return {
        "workspace": workspace,
        "trace": [{
            "round": state["round_id"],
            "stage": "build_workspace",
            "timestamp": datetime.now().isoformat(),
            "message": f"Workspace built: {len(recalled_packs)} recalled + {len(existing_packs)} existing packs",
        }],
    }

@trace_node("tool_loop")
async def tool_loop_node(state:ResearchState)-> ResearchStateUpdate:
    """å·¥å…·å¾ªç¯èŠ‚ç‚¹ï¼šæ•´åˆäº†å†³å®šè¡ŒåŠ¨å’Œæ‰§è¡Œå·¥å…·çš„åŠŸèƒ½ã€‚
    å¤§å‹è¯­è¨€æ¨¡å‹è‡ªä¸»å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·ï¼ˆå¤šæ­¥å¾ªç¯ï¼‰ã€‚
    å·¥å…·è¾“å‡ºè¢«å‹ç¼©æˆè¯æ®åŒ…ï¼ˆæ¸è¿›å¼å±•ç¤ºï¼‰ã€‚
    è¾“å…¥: workspace, open_questions, round_id, max_rounds, budget params
    è¾“å‡º: tool_results, evidence_packs, next_action, trace
    """
    logger.info(f"[Task {state['task_id']}] Entering tool loop for round {state['round_id']}")
    logger.debug(f"ğŸ“¥ Open questions: {len(state.get('open_questions', []))}, Workspace size: {len(state.get('workspace', ''))} chars")
    llm = make_llm(model="qwen3-max")
    llm_with_tools = llm.bind_tools(TOOLS)

    new_tool_results: List[Dict[str, Any]] = []
    new_packs: List[EvidencePack] = []  # Store as Dict to match state typing

    workspace = state.get("workspace", "")
    open_questions = state.get("open_questions", [])
    
    max_steps = state.get("max_tool_steps_per_round", 3)
    max_per_step = state.get("max_packs_per_step", 3)
    max_total = state.get("max_total_packs_per_round", 20)
    max_chars = state.get("max_workspace_chars", 8000)

    next_action = "continue"
    reason = None
    for step in range(max_steps):
        remaining_budget = max_total - len(new_packs)
        
        # è·å–æ—¶é—´èŒƒå›´ä¿¡æ¯
        task_spec = state.get("task_spec", {})
        time_range = task_spec.get("time_range", {})
        time_desc = time_range.get("description", "ä¸é™")
        start_year = time_range.get("start_year")
        end_year = time_range.get("end_year")
        priority = time_range.get("priority", "all")
        
        time_filter_hint = ""
        if start_year and end_year:
            time_filter_hint = f"\n**æ—¶é—´èŒƒå›´ï¼š{start_year}-{end_year}å¹´ï¼ˆ{time_desc}ï¼‰**"
            if priority == "latest":
                time_filter_hint += "\nâš ï¸ ä¼˜å…ˆæœç´¢æœ€æ–°å†…å®¹ï¼ˆæŒ‰æ—¶é—´é™åºï¼‰"
        
        # åŠ¨æ€è·å–å½“å‰æ—¥æœŸ
        current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
        
        # System prompt with budget constraints
        system_prompt = f"""ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ï¼Œå¯ä»¥è°ƒç”¨å·¥å…·æ”¶é›†è¯æ®ã€‚å½“å‰æ—¥æœŸï¼š{current_date}ã€‚{time_filter_hint}

**å½“å‰å·¥ä½œç©ºé—´ï¼ˆç²¾ç®€ï¼‰ï¼š**
{workspace}

**å¾…è§£å†³é—®é¢˜ï¼š**
{chr(10).join(f"{i+1}. {q}" for i, q in enumerate(open_questions))}

**é¢„ç®—ï¼š** æ­¥éª¤ {step+1}/{max_steps}ï¼Œè¯æ®åŒ… {len(new_packs)}/{max_total}ï¼Œå‰©ä½™ {remaining_budget}ï¼Œå·¥ä½œç©ºé—´é™åˆ¶ {max_chars} å­—ç¬¦

**å¯ç”¨å·¥å…·**ï¼š
- arxiv_search: æœç´¢å­¦æœ¯è®ºæ–‡
- github_search_repos: æœç´¢GitHubä»“åº“
- github_search_code: æœç´¢ä»£ç ç‰‡æ®µ
- web_search: ç½‘ç»œæœç´¢ï¼ˆè·å–ç›¸å…³ç½‘é¡µé“¾æ¥ï¼‰
- web_visit: è®¿é—®å…·ä½“ç½‘é¡µå¹¶æå–æ­£æ–‡å†…å®¹
- content_extract: ä»é•¿æ–‡æœ¬ä¸­æå–ä¸é—®é¢˜ç›¸å…³çš„è¯æ®

**å·¥å…·ä½¿ç”¨è§„åˆ™ï¼š**
- ä»…åœ¨éœ€è¦å¤–éƒ¨è¯æ®å›ç­”å¾…è§£å†³é—®é¢˜æ—¶è°ƒç”¨å·¥å…·
- æ¯æ¬¡è°ƒç”¨å·¥å…·å‰ï¼Œæ˜ç¡®ï¼š"æˆ‘è¦è§£å†³å¾…è§£å†³é—®é¢˜ä¸­çš„ç¬¬Nä¸ªé—®é¢˜"
- æ¨èæµç¨‹ï¼šweb_searchæ‰¾åˆ°ç›¸å…³é“¾æ¥ â†’ web_visitè®¿é—®é¡µé¢ â†’ content_extractæå–è¯æ®
- å¯¹äºå­¦æœ¯é—®é¢˜ä¼˜å…ˆä½¿ç”¨ arxiv_search
- å¯¹äºä»£ç /æŠ€æœ¯å®ç°ä¼˜å…ˆä½¿ç”¨ github_search
- å¯¹äºæœ€æ–°èµ„è®¯/åšå®¢ä¼˜å…ˆä½¿ç”¨ web_search + web_visit
- å…³æ³¨é«˜è´¨é‡æ¥æºï¼ˆæœ€æ–°è®ºæ–‡ã€çƒ­é—¨ä»“åº“ã€æƒå¨ç½‘ç«™ï¼‰
- æœç´¢æ—¶ä½¿ç”¨ä¸­æ–‡æˆ–è‹±æ–‡å…³é”®è¯
{f"- æ—¶é—´è¿‡æ»¤ï¼šä¼˜å…ˆæœç´¢ {start_year}-{end_year} å¹´çš„å†…å®¹" if start_year else ""}

**è¾“å‡ºè§„åˆ™ï¼ˆç»“æ„åŒ–ï¼‰ï¼š**
- å¦‚æœå°†è°ƒç”¨å·¥å…·ï¼šå…ˆè¾“å‡º JSON è¡ŒåŠ¨è®¡åˆ’ï¼Œå†ç›´æ¥è°ƒç”¨
  JSON æ ¼å¼ï¼š{{{{"action": "TOOL", "tool_name": "å·¥å…·å", "tool_args": {{}}, "target_question_id": N, "rationale": "è§£é‡Šä¸ºä»€ä¹ˆè°ƒç”¨"}}}}
- å¦‚æœä¸è°ƒç”¨ä»»ä½•å·¥å…·ï¼šåªè¿”å›æœ‰æ•ˆ JSONï¼š
  {{{{"action": "STOP", "reason": "è¯´æ˜ä¸ºä»€ä¹ˆåœæ­¢"}}}}
"""
        messages = [HumanMessage(content=system_prompt)]
        response = await llm_with_tools.ainvoke(messages)
        
        # === è§£æç»“æ„åŒ– action ===
        action_plan = None
        if hasattr(response, "content") and response.content:
            try:
                # å°è¯•è§£æ JSON
                content_str = _extract_text_from_response(response)
                # æŸ¥æ‰¾ JSON å—
                import re
                json_match = re.search(r'\{.*?"action".*?\}', content_str, re.DOTALL)
                if json_match:
                    action_plan = json.loads(json_match.group(0))
                    logger.debug(f"Parsed action plan: {action_plan}")
            except Exception as e:
                logger.debug(f"Failed to parse action JSON: {e}")
        
        if hasattr(response,"tool_calls") and response.tool_calls:
            # ç¡®å®šç›®æ ‡é—®é¢˜ï¼ˆä¼˜å…ˆä½¿ç”¨ action_planï¼Œå¦åˆ™ç”¨ç®€å•æ­£åˆ™ï¼‰
            target_question = ""
            if action_plan and "target_question_id" in action_plan:
                idx = action_plan["target_question_id"] - 1
                if 0 <= idx < len(open_questions):
                    target_question = open_questions[idx]
            else:
                # Fallback: ç®€å•è§£æ
                if hasattr(response, "content") and response.content:
                    import re
                    match = re.search(r'(?:è§£å†³|å›ç­”|å¤„ç†).*?ç¬¬(\d+).*?é—®é¢˜', str(response.content))
                    if match and open_questions:
                        idx = int(match.group(1)) - 1
                        if 0 <= idx < len(open_questions):
                            target_question = open_questions[idx]
            
            tool_node = ToolNode(TOOLS)
            tool_messages = await tool_node.ainvoke({"messages": [response]})
            for msg in tool_messages.get("messages", []):
                    if hasattr(msg, "content"):
                        try:
                            res = json.loads(msg.content) if isinstance(msg.content, str) else msg.content
                        except Exception:
                            res = msg.content
                        
                        tool_name = msg.name if hasattr(msg, "name") else "unknown"
                        
                        new_tool_results.append({
                            "tool": tool_name,
                            "query": getattr(response, "content", ""),
                            "result": res,
                            "target_question": target_question or open_questions[0] if open_questions else "",  # è®°å½•ç›®æ ‡é—®é¢˜
                        })
                        # [PROGRESSIVE DISCLOSURE] Convert items to compact EvidencePacks
                        if isinstance(res, list):
                            for item in res[:max_per_step]:
                                if isinstance(item, dict):
                                    pack = _make_pack_from_item(item, tool_name)
                                    new_packs.append(pack)
                                    
                                    if len(new_packs) >= max_total:
                                        break
            show_packs = new_packs[-max_total:]
            evidence_summary = "\n## è¯æ®åŒ…ï¼ˆæœ¬æ­¥éª¤ï¼‰\n"
            for p in show_packs:
                evidence_summary += f"- [{p.get('pack_id', '')}] {p.get('source', '')}:{p.get('ref', '')} - {p.get('snippet', '')[:120]}\n"
                for kp in p.get("key_points", [])[:3]:
                    evidence_summary += f"  * {kp}\n"           
            workspace = f"""## ç ”ç©¶é—®é¢˜
    {state.get('query')}

    ## å½“å‰æ€»ç»“
    {state.get('summary') or 'æš‚æ— æ€»ç»“ã€‚'}

    ## å¾…è§£å†³é—®é¢˜
    {chr(10).join(f"- {q}" for q in open_questions)}

    {evidence_summary}"""
        
            if len(workspace) > max_chars:
                workspace = workspace[:max_chars] + "\n... (truncated)"
            if len(new_packs) >= max_total:
                break
                
            continue  # Next step
        
        # No tool calls: expect JSON control response
        content = _extract_text_from_response(response)
        try:
            control = json.loads(content)
        except Exception:
            control = {}
        
        if control.get("type") == "ANSWER":
            next_action = "answer"
            reason = control.get("reason")
            break
        elif control.get("type") == "STOP":
            next_action = "stop"
            reason = control.get("reason")
            break
        # Otherwise keep looping
    
    trace_msg = f"Tool loop finished: {step+1} steps, {len(new_packs)} packs, next_action={next_action}"
    
    return {
        "tool_results": new_tool_results,
        "evidence_packs": new_packs,
        "new_evidence_packs": new_packs,    # æœ¬è½®ä¸“ç”¨ï¼ˆreplaceï¼‰
        "next_action": next_action,
        "trace": [{
            "round": state["round_id"],
            "stage": "tool_loop",
            "timestamp": datetime.now().isoformat(),
            "message": trace_msg,
            "data": {"reason": reason, "steps": step+1, "packs": len(new_packs)},
        }],
    }


@trace_node("materialize_round_cache")
async def materialize_round_cache_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: ç‰©åŒ–æœ¬è½® Session Cacheï¼ˆhitâ†’docâ†’note å‡çº§ï¼‰
    
    å…³é”®åŠŸèƒ½ï¼š
    1. å°† tool_results ä¸­çš„ hit ç»“æœå‡çº§ä¸º docï¼ˆè°ƒç”¨ web_visit/get_readme ç­‰ï¼‰
    2. å¯¹æ¯ä¸ª doc è¿›è¡Œåˆ†å—ï¼ˆchunkingï¼‰å¹¶å­˜å…¥ session_docs
    3. è°ƒç”¨ content_extract ç”Ÿæˆç»“æ„åŒ–è¯æ®ç¬”è®°ï¼ˆResearchNoteï¼‰
    4. å¤„ç†å›¾ç‰‡è§£æé“¾è·¯ï¼ˆfile_parse images â†’ image_parseï¼‰
    5. æ›´æ–° workspace å±•ç¤º
    
    è¾“å…¥: tool_results, session_cache, open_questions
    è¾“å‡º: session_cache, round_doc_ids, round_notes, all_notes, workspace
    """
    logger.info(f"[Task {state['task_id']}] Materializing round cache for round {state['round_id']}")
    logger.debug(f"ğŸ“¥ Tool results: {len(state.get('tool_results', []))}")
    
    from ..tools.langchain_tool import TOOLS
    from ..tools.content_extract_tool import ContentExtractClient
    from langchain_core.runnables import RunnableConfig
    
    # åˆå§‹åŒ–æˆ–æ¢å¤ SessionCache
    cache_data = state.get("session_cache", {})
    cache = SessionCache.from_state_dict(cache_data) if cache_data else SessionCache()
    
    chunker = get_default_chunker()
    embedding_provider = get_embedding_provider()
    
    # è·å–å½“å‰å¾…è§£å†³é—®é¢˜
    open_questions = state.get("open_questions", [state["query"]])
    goal_text = "\n".join(f"{i+1}. {q}" for i, q in enumerate(open_questions[:5]))
    
    # å‡†å¤‡ content_extract client
    llm = make_llm(model="qwen3-max", temperature=0)
    async def _generate(prompt: str) -> str:
        resp = await llm.ainvoke([HumanMessage(content=prompt)])
        return _extract_text_from_response(resp)
    
    content_extractor = ContentExtractClient(generate=_generate)
    
    # å‡†å¤‡ image_parseï¼ˆå¦‚éœ€è¦ï¼‰
    from ..tools.image_parse_tool import get_image_parser
    image_parser = get_image_parser()
    
    round_doc_ids: List[str] = []
    round_notes: List[ResearchNote] = []
    
    # === Phase 1: è§£æ tool_resultsï¼Œå‡çº§ hit â†’ doc ===
    docs_to_process: List[Dict[str, Any]] = []
    
    for result in state["tool_results"]:
        tool_name = result.get("tool", "")
        target_question = result.get("target_question", "")
        data = result.get("result", [])
        
        if not isinstance(data, list):
            continue
        
        for item in data:
            if not isinstance(item, dict):
                continue
            
            doc_candidate: Optional[Dict[str, Any]] = None
            
            # Case 0: æœ¬åœ°çŸ¥è¯†åº“ hitï¼ˆå ä½ï¼‰
            if "kb_id" in item and "doc_id" in item:
                doc_candidate = {
                    "type": "kb_hit",
                    "kb_id": item["kb_id"],
                    "doc_id": item["doc_id"],
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "target_question": target_question
                }
            
            # Case 1: web_search hit â†’ éœ€è¦ web_visit
            if tool_name == "web_search" and "url" in item:
                # æ ‡è®°ä¸ºéœ€è¦å‡çº§çš„ hit
                doc_candidate = {
                    "type": "web_hit",
                    "url": item["url"],
                    "title": item.get("title", ""),
                    "snippet": item.get("snippet", ""),
                    "target_question": target_question
                }
            
            # Case 2: arxiv_search hit â†’ ä½¿ç”¨æ‘˜è¦ä½œä¸º doc
            elif "arxiv_id" in item:
                doc_candidate = {
                    "type": "arxiv_doc",
                    "doc_id": f"arxiv_{item['arxiv_id']}",
                    "source": "arxiv",
                    "ref": item["arxiv_id"],
                    "url": item.get("abs_url") or item.get("url"),
                    "title": item.get("title", ""),
                    "text": item.get("summary", ""),
                    "origin": "web",
                    "target_question": target_question
                }
            
            # Case 3: github_search_repos hit â†’ éœ€è¦ get_readme
            elif "full_name" in item:
                doc_candidate = {
                    "type": "github_hit",
                    "full_name": item["full_name"],
                    "url": item.get("url", ""),
                    "title": item.get("name", ""),
                    "description": item.get("description", ""),
                    "target_question": target_question
                }
            
            # Case 4: web_visit å·²ç»è¿”å› doc
            elif "text" in item and "url" in item and len(item.get("text", "")) > 200:
                doc_candidate = {
                    "type": "web_doc",
                    "doc_id": f"web_{hash_text(item['url'])}",
                    "source": "web",
                    "ref": item["url"],
                    "url": item["url"],
                    "title": item.get("title", ""),
                    "text": item["text"],
                    "origin": "web",
                    "target_question": target_question
                }
            
            if doc_candidate:
                docs_to_process.append(doc_candidate)
    
    logger.info(f"Found {len(docs_to_process)} doc candidates to materialize")
    
    # === Phase 2: å‡çº§ hit â†’ docï¼ˆè°ƒç”¨çœŸå®å·¥å…·ï¼‰===
    materialized_docs: List[RawDoc] = []
    
    # å‡†å¤‡å·¥å…·æ˜ å°„
    tool_map = {tool.name: tool for tool in TOOLS}
    runnable_config = RunnableConfig(
        configurable={"task_id": state["task_id"]},
        run_name=f"materialize_tools_round_{state['round_id']}"
    )
    
    for doc_info in docs_to_process[:10]:  # é™åˆ¶æ•°é‡é¿å…è¶…æ—¶
        try:
            doc_id = ""
            doc_text = ""
            doc_source = doc_info.get("source", "web")
            doc_ref = doc_info.get("ref", "")
            doc_url = doc_info.get("url")
            doc_title = doc_info.get("title", "")
            doc_origin = doc_info.get("origin", "web")
            degraded = False
            
            if doc_info["type"] == "kb_hit":
                # è°ƒç”¨æœ¬åœ° KB fetchï¼ˆå ä½ï¼‰
                logger.debug(f"Calling local_kb_fetch for: {doc_info['doc_id']}")
                try:
                    from ..tools.local_kb_tool import local_kb_fetch
                    result = await local_kb_fetch(
                        doc_id=doc_info["doc_id"],
                        kb_id=doc_info.get("kb_id")
                    )
                    doc_id = f"kb_{doc_info['kb_id']}_{doc_info['doc_id']}"
                    doc_text = result["text"]
                    doc_source = "local_kb"
                    doc_ref = doc_info["doc_id"]
                    doc_title = result.get("title", doc_info["title"])
                    doc_origin = "local"  # æœ¬åœ°æ–‡æ¡£ä¼˜å…ˆçº§é«˜
                except NotImplementedError:
                    logger.info(f"Local KB not implemented, using snippet for {doc_info['doc_id']}")
                    doc_id = f"kb_{doc_info.get('kb_id', 'unknown')}_{doc_info['doc_id']}"
                    doc_text = f"[Local KB Hit]\n{doc_info['title']}\n\n{doc_info['snippet']}"
                    doc_source = "local_kb"
                    doc_ref = doc_info["doc_id"]
                    doc_origin = "local"
                    degraded = True
                except Exception as e:
                    logger.warning(f"local_kb_fetch failed: {e}")
                    continue
            
            elif doc_info["type"] == "web_hit":
                # è°ƒç”¨çœŸå®çš„ web_visit å·¥å…·
                logger.debug(f"Calling web_visit for: {doc_info['url']}")
                try:
                    if "web_visit" in tool_map:
                        result = await tool_map["web_visit"].ainvoke(
                            {"url": doc_info["url"]},
                            config=runnable_config
                        )
                        if isinstance(result, dict) and "text" in result:
                            doc_id = f"web_{hash_text(doc_info['url'])}"
                            doc_text = result["text"]
                            doc_source = "web"
                            doc_ref = doc_info["url"]
                            doc_title = result.get("title", doc_info["title"])
                        else:
                            raise ValueError("Invalid web_visit result")
                    else:
                        raise ValueError("web_visit tool not available")
                except Exception as e:
                    logger.warning(f"web_visit failed for {doc_info['url']}: {e}, using snippet")
                    doc_id = f"web_{hash_text(doc_info['url'])}"
                    doc_text = f"[Title] {doc_info['title']}\n\n{doc_info['snippet']}"
                    doc_source = "web"
                    doc_ref = doc_info["url"]
                    degraded = True
                
            elif doc_info["type"] == "github_hit":
                # è°ƒç”¨ github_get_readme å·¥å…·
                logger.debug(f"Calling github_get_readme for: {doc_info['full_name']}")
                try:
                    if "github_get_readme" in tool_map:
                        result = await tool_map["github_get_readme"].ainvoke(
                            {"repo": doc_info["full_name"]},
                            config=runnable_config
                        )
                        if isinstance(result, dict) and "content" in result:
                            doc_id = f"github_{hash_text(doc_info['full_name'])}"
                            doc_text = result["content"]
                            doc_source = "github"
                            doc_ref = doc_info["full_name"]
                            doc_title = doc_info["title"]
                        else:
                            raise ValueError("Invalid github_get_readme result")
                    else:
                        raise ValueError("github_get_readme tool not available")
                except Exception as e:
                    logger.warning(f"github_get_readme failed for {doc_info['full_name']}: {e}, using description")
                    doc_id = f"github_{hash_text(doc_info['full_name'])}"
                    doc_text = f"[Repo] {doc_info['full_name']}\n\n{doc_info['description']}"
                    doc_source = "github"
                    doc_ref = doc_info["full_name"]
                    degraded = True
                
            elif doc_info["type"] in ("arxiv_doc", "web_doc"):
                # å·²ç»æ˜¯ doc
                doc_id = doc_info["doc_id"]
                doc_text = doc_info["text"]
                doc_source = doc_info.get("source", "web")
                doc_ref = doc_info.get("ref", "")
                doc_url = doc_info.get("url")
                doc_title = doc_info.get("title", "")
            else:
                continue
            
            if not doc_text.strip():
                logger.warning(f"Empty doc text for {doc_id}, skipping")
                continue
            
            # åˆ†å—
            chunks = chunker.split_text_to_chunks(doc_text, doc_id)
            
            # åˆ›å»º RawDoc
            raw_doc = RawDoc(
                doc_id=doc_id,
                source=doc_source,
                ref=doc_ref,
                url=doc_url,
                title=doc_title,
                fetched_at=datetime.now().isoformat(),
                text=doc_text,
                chunks=chunks,
                metadata={"target_question": doc_info.get("target_question", "")},
                origin=doc_origin,
                degraded=degraded
            )
            
            cache.add_doc(raw_doc)
            round_doc_ids.append(doc_id)
            materialized_docs.append(raw_doc)
            
        except Exception as e:
            logger.error(f"Failed to materialize doc: {e}")
            continue
    
    logger.info(f"Materialized {len(materialized_docs)} docs with total {sum(len(d.chunks) for d in materialized_docs)} chunks")
    
    # === Phase 3: å¯¹æ¯ä¸ª doc è°ƒç”¨ content_extract ç”Ÿæˆ ResearchNote ===
    for raw_doc in materialized_docs:
        try:
            # è°ƒç”¨ content_extract
            extract_result = await content_extractor.extract(
                question=goal_text,
                document_text=raw_doc.text[:10000],  # é™åˆ¶é•¿åº¦
                max_quotes=5
            )
            
            if "error" in extract_result:
                logger.warning(f"content_extract failed for {raw_doc.doc_id}: {extract_result['error']}")
                # Fallback: ä½¿ç”¨ç®€å•æ‘˜è¦
                key_points = [raw_doc.text[:200]]
                evidence_quotes = []
            else:
                key_points = extract_result.get("key_points", [])
                evidence_quotes = extract_result.get("evidence_quotes", [])
            
            # ç”Ÿæˆ snippet
            snippet = " | ".join(key_points[:3])[:300]
            
            # åˆ›å»º ResearchNote
            pack_id = generate_evidence_id(raw_doc.source, raw_doc.ref, hash_text(snippet))
            note: ResearchNote = {
                "pack_id": pack_id,
                "source": raw_doc.source,
                "ref": raw_doc.ref,
                "url": raw_doc.url,
                "title": raw_doc.title,
                "snippet": snippet,
                "key_points": key_points[:10],
                "evidence_quotes": evidence_quotes[:8],
                "raw_pointer": raw_doc.doc_id,
                "confidence": "medium",
                "relevance": "",
                "media_type": "text",
                "image_url": None,
                "fetched_at": raw_doc.fetched_at,
                "target_question": raw_doc.metadata.get("target_question")
            }
            
            cache.add_note(note)
            round_notes.append(note)
            
        except Exception as e:
            logger.error(f"Failed to generate note for {raw_doc.doc_id}: {e}")
            continue
    
    logger.info(f"Generated {len(round_notes)} research notes")
    
    # === Phase 4: å¤„ç†å›¾ç‰‡ï¼ˆå¦‚ tool_results ä¸­æœ‰ imagesï¼‰===
    # å›¾ç‰‡è§£æé“¾è·¯ï¼šfile_parse(extract_images=True) â†’ images_base64 â†’ image_parse
    for result in state["tool_results"]:
        data = result.get("result", [])
        if isinstance(data, dict) and "images_base64" in data:
            images_b64 = data["images_base64"][:3]  # é™åˆ¶æ•°é‡
            for idx, img_b64 in enumerate(images_b64):
                try:
                    data_url = f"data:image/png;base64,{img_b64}"
                    question = "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹ï¼ŒåŒ…æ‹¬ä¸»è¦å¯¹è±¡ã€æ–‡å­—ã€å›¾è¡¨ç±»å‹ç­‰ã€‚"
                    
                    img_result = await image_parser.parse(question=question, image_url=data_url)
                    
                    description = img_result.get("description", "")
                    extracted_text = img_result.get("extracted_text", "")
                    analysis = img_result.get("analysis", "")
                    
                    # ç»„åˆä¸ºæ–‡æœ¬
                    img_text = f"{description}\n\næå–æ–‡å­—ï¼š{extracted_text}\n\nåˆ†æï¼š{analysis}"
                    
                    # åˆ›å»ºå›¾ç‰‡ doc
                    img_doc_id = f"image_{hash_text(img_b64[:100])}"
                    img_doc = RawDoc(
                        doc_id=img_doc_id,
                        source="local_file",
                        ref=f"image_{idx}",
                        url=data_url,
                        title=f"Image {idx + 1}",
                        fetched_at=datetime.now().isoformat(),
                        text=img_text,
                        chunks=[],  # å›¾ç‰‡ä¸åˆ†å—
                        metadata={"image_index": idx},
                        origin="local"
                    )
                    
                    cache.add_doc(img_doc)
                    round_doc_ids.append(img_doc_id)
                    
                    # åˆ›å»ºå›¾ç‰‡ note
                    img_pack_id = generate_evidence_id("image", img_doc_id, hash_text(description))
                    img_note: ResearchNote = {
                        "pack_id": img_pack_id,
                        "source": "local_file",
                        "ref": f"image_{idx}",
                        "url": data_url,
                        "title": f"Image {idx + 1}",
                        "snippet": description[:300],
                        "key_points": [description[:200]],
                        "evidence_quotes": [],
                        "raw_pointer": img_doc_id,
                        "confidence": "medium",
                        "relevance": "",
                        "media_type": "image",
                        "image_url": data_url,
                        "fetched_at": img_doc.fetched_at,
                        "target_question": None
                    }
                    
                    cache.add_note(img_note)
                    round_notes.append(img_note)
                    
                except Exception as e:
                    logger.error(f"Failed to parse image {idx}: {e}")
                    continue
    
    # === Phase 5: æ›´æ–° workspaceï¼ˆåªå±•ç¤ºç´§å‡‘ç¬”è®°ï¼Œä¸åŒ…å«åŸæ–‡ï¼‰===
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    
    workspace_parts = [
        f"## ç ”ç©¶é—®é¢˜\n{state['query']}",
        f"\n## å½“å‰æ—¶é—´\n{current_date}",
        f"\n## å½“å‰æ€»ç»“\n{state.get('summary') or 'æš‚æ— æ€»ç»“ã€‚'}",
    ]
    
    if state.get("open_questions"):
        workspace_parts.append(
            f"\n## å¾…è§£å†³é—®é¢˜\n" + "\n".join(f"{i+1}. {q}" for i, q in enumerate(state["open_questions"]))
        )
    
    if state.get("findings"):
        workspace_parts.append(
            f"\n## æœ€è¿‘å‘ç°\n" + "\n".join(f"- {f}" for f in state["findings"][-5:])
        )
    
    # è¯æ®åŒ…å±•ç¤ºï¼ˆç´§å‡‘ï¼‰
    all_notes = list(state.get("all_notes", [])) + round_notes
    if round_notes:
        evidence_summary = "\n## æœ¬è½®è¯æ®ç¬”è®°ï¼ˆç´§å‡‘ï¼‰\n"
        for note in round_notes[:10]:
            if note.get("media_type") == "image":
                evidence_summary += f"- [IMAGE] [{note['pack_id']}] {note['snippet'][:120]}\n"
                evidence_summary += f"  ğŸ–¼ï¸ URL: {note.get('image_url', '')[:50]}...\n"
            else:
                evidence_summary += f"- [{note['pack_id']}] {note['source']}:{note['ref']} - {note['snippet'][:120]}\n"
                for kp in note.get("key_points", [])[:2]:
                    evidence_summary += f"  * {kp[:100]}\n"
        workspace_parts.append(evidence_summary)
    
    workspace = "\n".join(workspace_parts)
    
    # é™åˆ¶é•¿åº¦
    max_chars = state.get("max_workspace_chars", 8000)
    if len(workspace) > max_chars:
        workspace = workspace[:max_chars] + "\n... (truncated)"
    
    # åºåˆ—åŒ– session_cache
    session_cache_dict = cache.to_state_dict()
    
    # æ·»åŠ åˆ° persist_queue
    persist_queue_additions = [note["pack_id"] for note in round_notes]
    
    return {
        "session_cache": session_cache_dict,
        "round_doc_ids": round_doc_ids,
        "round_notes": round_notes,
        "all_notes": round_notes,  # ä¼šè¢« add åˆ°ç´¯è®¡åˆ—è¡¨
        "workspace": workspace,
        "persist_queue": persist_queue_additions,
        "trace": [{
            "round": state["round_id"],
            "stage": "materialize_round_cache",
            "timestamp": datetime.now().isoformat(),
            "message": f"Materialized {len(materialized_docs)} docs, {len(round_notes)} notes",
            "data": {"docs": len(materialized_docs), "notes": len(round_notes), "images": sum(1 for n in round_notes if n.get("media_type") == "image")}
        }]
    }


@trace_node("normalize_evidence")
async def normalize_evidence_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: æ™ºèƒ½æå–è¯æ®å¹¶å­˜å‚¨åˆ° Milvus
    
    ä½¿ç”¨LLMæ ¹æ®å½“å‰å¾…è§£å†³é—®é¢˜æ™ºèƒ½æå–è¯æ®çš„æ ¸å¿ƒä¿¡æ¯
    è¾“å…¥: tool_results, open_questions
    è¾“å‡º: evidence_ids (è¿½åŠ )
    """
    logger.info(f"[Task {state['task_id']}] Normalizing evidence with LLM extraction")
    logger.debug(f"ğŸ“¥ Tool results count: {len(state.get('tool_results', []))}")
    
    embedding_provider = get_embedding_provider()
    writer = get_writer()
    llm = make_llm(model="qwen3-max", temperature=0)
    
    # è·å–å½“å‰å¾…è§£å†³é—®é¢˜ç”¨äºæ™ºèƒ½æå–
    open_questions = state.get("open_questions", [state["query"]])
    goal_text = "\n".join(f"- {q}" for q in open_questions[:5])
    
    new_evidence_ids = []
    chunks = []
    
    # LLMæå–æç¤ºæ¨¡æ¿
    extractor_prompt = ChatPromptTemplate.from_messages([
        ("system", """è¯·å¤„ç†ä»¥ä¸‹å†…å®¹ï¼Œå¹¶æ ¹æ®å½“å‰çš„ç ”ç©¶ç›®æ ‡æå–å…³é”®ä¿¡æ¯ã€‚

**æå–æŒ‡å—**ï¼š
1. **ç†ç”± (rationale)**: ä¸ºä»€ä¹ˆè¿™æ®µå†…å®¹ä¸ç›®æ ‡ç›¸å…³ï¼Ÿï¼ˆ1-2å¥ï¼‰
2. **è¯æ® (evidence)**: æå–åŸæ–‡ä¸­æœ€æ ¸å¿ƒçš„æ®µè½ï¼Œä¿ç•™åŸæ–‡ç»†èŠ‚ï¼ˆæ•°æ®ã€ç»“è®ºã€æ–¹æ³•ç­‰ï¼‰ï¼Œä¸è¦è¿‡åº¦æ‘˜è¦ï¼ˆæœ€å¤š2000å­—ç¬¦ï¼‰
3. **æ‘˜è¦ (summary)**: å¯¹æå–çš„ä¿¡æ¯è¿›è¡Œé€»è¾‘æ¦‚æ‹¬ï¼ˆ3-5å¥ï¼Œ200å­—ç¬¦å†…ï¼‰
4. **å…³é”®ç‚¹ (key_points)**: æœ€é‡è¦çš„3-5ä¸ªè¦ç‚¹ï¼ˆæ¯ä¸ªä¸è¶…è¿‡100å­—ç¬¦ï¼‰

è¿”å› JSON æ ¼å¼ï¼š
{{
    "rationale": "...",
    "evidence": "...",
    "summary": "...",
    "key_points": ["...", "..."]
}}"""),
        ("human", """## å½“å‰å¾…è§£å†³é—®é¢˜ï¼ˆç ”ç©¶ç›®æ ‡ï¼‰
{goal}

## åŸå§‹å†…å®¹
æ¥æºï¼š{source}
æ ‡é¢˜ï¼š{title}
å†…å®¹ï¼š
{content}

è¯·æå–å…³é”®ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰ï¼š""")
    ])
    
    for result in state["tool_results"]:
        tool_name = result.get("tool", "")
        target_question = result.get("target_question", "")  # ä»tool_loopä¼ æ¥çš„ç›®æ ‡é—®é¢˜
        data = result.get("result", [])
        
        if not isinstance(data, list):
            continue
        
        for item in data:
            if not isinstance(item, dict):
                continue
            
            # arXiv paper
            if "arxiv_id" in item:
                ev_id = generate_evidence_id("arxiv", item["arxiv_id"], hash_text(item.get("summary", "")))
                if ev_id in state.get("evidence_ids", []):
                    continue
                
                # LLMæ™ºèƒ½æå–
                raw_content = item.get("summary", "")[:10000]
                try:
                    extract_response = await (extractor_prompt | llm).ainvoke({
                        "goal": goal_text,
                        "source": "arxiv",
                        "title": item.get("title", ""),
                        "content": raw_content,
                    })
                    extracted = json.loads(_extract_text_from_response(extract_response))
                except Exception as e:
                    logger.warning(f"LLM extraction failed: {e}, using fallback")
                    extracted = {
                        "rationale": "ç›¸å…³è®ºæ–‡",
                        "evidence": raw_content[:2000],
                        "summary": raw_content[:200],
                        "key_points": [raw_content[:100]]
                    }
                
                embedding = await embedding_provider.embed(extracted["evidence"])
                chunks.append({
                    "id": ev_id,
                    "embedding": embedding,
                    "text": extracted["evidence"],
                    "source": "arxiv",
                    "ref": item["arxiv_id"],
                    "snippet": extracted["summary"],
                    "task_id": state["task_id"],
                    "round_id": state["round_id"],
                    "metadata": {
                        "title": item.get("title", ""),
                        "authors": item.get("authors", []),
                        "url": item.get("abs_url", ""),
                        "rationale": extracted["rationale"],
                        "key_points": extracted["key_points"],
                        "target_question": target_question,
                    },
                })
                new_evidence_ids.append(ev_id)
            
            # GitHub repo
            elif "full_name" in item:
                description = item.get("description", "") or item.get("name", "")
                readme = item.get("readme", "")[:5000]  # åŒ…å«READMEå†…å®¹
                ev_id = generate_evidence_id("github", item["full_name"], hash_text(description + readme))
                if ev_id in state.get("evidence_ids", []):
                    continue
                
                # LLMæ™ºèƒ½æå–
                raw_content = f"{description}\n\nREADME:\n{readme}"
                try:
                    extract_response = await (extractor_prompt | llm).ainvoke({
                        "goal": goal_text,
                        "source": "github",
                        "title": item.get("full_name", ""),
                        "content": raw_content[:10000],
                    })
                    extracted = json.loads(_extract_text_from_response(extract_response))
                except Exception as e:
                    logger.warning(f"LLM extraction failed: {e}, using fallback")
                    extracted = {
                        "rationale": "ç›¸å…³ä»“åº“",
                        "evidence": raw_content[:2000],
                        "summary": description[:200],
                        "key_points": [description[:100]]
                    }
                
                embedding = await embedding_provider.embed(extracted["evidence"])
                chunks.append({
                    "id": ev_id,
                    "embedding": embedding,
                    "text": extracted["evidence"],
                    "source": "github",
                    "ref": item["full_name"],
                    "snippet": extracted["summary"],
                    "task_id": state["task_id"],
                    "round_id": state["round_id"],
                    "metadata": {
                        "name": item.get("name", ""),
                        "stars": item.get("stars", 0),
                        "url": item.get("url", ""),
                        "rationale": extracted["rationale"],
                        "key_points": extracted["key_points"],
                        "target_question": target_question,
                    },
                })
                new_evidence_ids.append(ev_id)
            
            # Webé¡µé¢ï¼ˆæ–°å¢æ”¯æŒï¼‰
            elif "text" in item and "url" in item:
                url = item["url"]
                ev_id = generate_evidence_id("web", url, hash_text(item.get("text", "")[:1000]))
                if ev_id in state.get("evidence_ids", []):
                    continue
                
                # LLMæ™ºèƒ½æå–
                raw_content = item.get("text", "")[:10000]
                try:
                    extract_response = await (extractor_prompt | llm).ainvoke({
                        "goal": goal_text,
                        "source": "web",
                        "title": item.get("title", ""),
                        "content": raw_content,
                    })
                    extracted = json.loads(_extract_text_from_response(extract_response))
                except Exception as e:
                    logger.warning(f"LLM extraction failed: {e}, using fallback")
                    extracted = {
                        "rationale": "ç›¸å…³ç½‘é¡µ",
                        "evidence": raw_content[:2000],
                        "summary": raw_content[:200],
                        "key_points": [raw_content[:100]]
                    }
                
                embedding = await embedding_provider.embed(extracted["evidence"])
                chunks.append({
                    "id": ev_id,
                    "embedding": embedding,
                    "text": extracted["evidence"],
                    "source": "web",
                    "ref": url,
                    "snippet": extracted["summary"],
                    "task_id": state["task_id"],
                    "round_id": state["round_id"],
                    "metadata": {
                        "title": item.get("title", ""),
                        "url": url,
                        "rationale": extracted["rationale"],
                        "key_points": extracted["key_points"],
                        "target_question": target_question,
                    },
                })
                new_evidence_ids.append(ev_id)
    
    if chunks:
        writer.write_evidence_batch(chunks)
    
    return {
        "evidence_ids": new_evidence_ids,
        "trace": [{
            "round": state["round_id"],
            "stage": "normalize_evidence",
            "timestamp": datetime.now().isoformat(),
            "message": f"Stored {len(new_evidence_ids)} new evidence items with LLM extraction",
        }],
    }


@trace_node("update_report")
async def update_report_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: æ›´æ–°ä¸­å¿ƒæŠ¥å‘Šï¼ˆé‡æ„ç‰ˆï¼šåªæ¶ˆè´¹ notesï¼‰
    
    è¾“å…¥: round_notes, all_notes, summary, findings, open_questions
    è¾“å‡º: summary (æ›´æ–°), findings (è¿½åŠ ), open_questions (æ›´æ–°)
    """
    logger.info(f"[Task {state['task_id']}] Updating central report from notes")
    logger.debug(f"ğŸ“¥ Round notes: {len(state.get('round_notes', []))}")
    
    # å¦‚æœæœ¬è½®æ²¡æœ‰æ–° notesï¼Œä¸æ›´æ–°
    if not state.get("round_notes"):
        logger.info("No new notes this round, skipping update_report")
        return {
            "trace": [{
                "round": state["round_id"],
                "stage": "update_report",
                "timestamp": datetime.now().isoformat(),
                "message": "No new notes to process",
            }],
        }
    
    # æ„å»º notes çš„ç´§å‡‘è¡¨ç¤ºï¼ˆkey_points + evidence_quotesï¼‰
    round_notes = state.get("round_notes", [])[:20]  # é™åˆ¶æ•°é‡
    evidence_text_lines = []
    
    from ..services.session_cache import safe_note_pack_id, safe_note_source, safe_note_evidence_quotes
    
    for note in round_notes:
        pack_id = safe_note_pack_id(note)
        source = safe_note_source(note)
        ref = note.get("ref", "")
        snippet = note.get("snippet", "")
        key_points = note.get("key_points", [])[:5]
        evidence_quotes = safe_note_evidence_quotes(note)[:3]
        
        kp_text = "\n".join(f"  - {kp}" for kp in key_points)
        eq_text = "\n".join(
            f"  â†’ [{eq.get('location_hint', '')}] {eq.get('quote', '')[:150]}"
            for eq in evidence_quotes
        )
        
        evidence_text_lines.append(
            f"[{pack_id}] {source}:{ref}\n"
            f"Snippet: {snippet}\n"
            f"Key Points:\n{kp_text}\n"
            f"Evidence Quotes:\n{eq_text}"
        )
    
    evidence_text = "\n\n".join(evidence_text_lines)
    
    llm = make_llm(model="qwen3-max")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """åˆ†ææ–°çš„ç ”ç©¶ç¬”è®°å¹¶æ›´æ–°æŠ¥å‘Šã€‚
åŸºäºç´§å‡‘çš„ç¬”è®°ï¼ˆå¸¦ pack_id å¼•ç”¨ï¼‰ï¼Œæä¾›ï¼š
1. æ–°å‘ç°ï¼ˆç®€æ´é™ˆè¿°åˆ—è¡¨ï¼Œå¿…é¡»å¼•ç”¨ pack_idï¼Œæ ¼å¼ï¼š[pack_id]ï¼‰
2. å“ªäº›å¾…è§£å†³é—®é¢˜ç°åœ¨å¯ä»¥å›ç­”äº†ï¼ˆåˆ—è¡¨ï¼‰
3. æ–°å‡ºç°çš„å¾…è§£å†³é—®é¢˜ï¼ˆåˆ—è¡¨ï¼‰
4. æ•´åˆæ–°æ—§å‘ç°çš„æ›´æ–°æ‘˜è¦

é‡è¦ï¼šæ¯ä¸ªå‘ç°å¿…é¡»åœ¨æ–¹æ‹¬å·ä¸­å¼•ç”¨è‡³å°‘ä¸€ä¸ª pack_idã€‚

è¿”å› JSONï¼š
{{
    "new_findings": ["åŸºäºè¯æ® [pack_id_123] çš„å‘ç°1", "å‘ç°2 [pack_id_456]"],
    "resolved_questions": ["å·²å›ç­”çš„é—®é¢˜"],
    "new_open_questions": ["æ–°å‡ºç°çš„é—®é¢˜"],
    "updated_summary": "å¼•ç”¨ [pack_id_xxx] è¯æ®çš„ç»¼åˆæ‘˜è¦"
}}"""),
        ("human", """å½“å‰æŠ¥å‘Šï¼š
æ‘˜è¦ï¼š{summary}
æœ€è¿‘å‘ç°ï¼š{findings}
å¾…è§£å†³é—®é¢˜ï¼š{open_questions}

æ–°ç¬”è®°ï¼š
{evidence}

è¯·æä¾›æŠ¥å‘Šæ›´æ–°ã€‚"""),
    ])
    
    chain = prompt | llm
    response = await chain.ainvoke({
        "summary": state.get("summary") or "None yet",
        "findings": state.get("findings", [])[-5:],
        "open_questions": state.get("open_questions", []),
        "evidence": evidence_text,
    })
    
    try:
        content = _extract_text_from_response(response)
        update = json.loads(content)
    except Exception:
        update = {"new_findings": [], "resolved_questions": [], "new_open_questions": [], "updated_summary": state.get("summary", "")}
    
    # Update open questions
    resolved = set(update.get("resolved_questions", []))
    remaining_questions = [q for q in state.get("open_questions", []) if q not in resolved]
    new_open_questions = remaining_questions + update.get("new_open_questions", [])
    
    return {
        "summary": update.get("updated_summary", state.get("summary", "")),
        "findings": update.get("new_findings", []),
        "open_questions": new_open_questions,
        "trace": [{
            "round": state["round_id"],
            "stage": "update_report",
            "timestamp": datetime.now().isoformat(),
            "message": f"Added {len(update.get('new_findings', []))} findings",
            "data": update,
        }],
    }


@trace_node("check_stop")
async def check_stop_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: æ£€æŸ¥åœæ­¢æ¡ä»¶ï¼ˆä½¿ç”¨LLMå†³ç­–ï¼‰
    
    è¾“å…¥: round_id, max_rounds, open_questions, findings, summary
    è¾“å‡º: next_action, stop_reason
    """
    logger.info(f"[Task {state['task_id']}] Checking stop conditions")
    logger.debug(f"ğŸ“¥ Round {state['round_id']}/{state['max_rounds']}, Open questions: {len(state.get('open_questions', []))}")
    
    # Hard stop conditions
    if state["round_id"] >= state["max_rounds"]:
        return {
            "next_action": "stop",
            "stop_reason": "max_rounds_reached",
            "trace": [{
                "round": state["round_id"],
                "stage": "check_stop",
                "timestamp": datetime.now().isoformat(),
                "message": "åœæ­¢ï¼šè¾¾åˆ°æœ€å¤§è½®æ¬¡",
            }],
        }
    
    # LLM-based decision
    current_date = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥")
    current_year = datetime.now().year
    
    llm = make_llm(model="qwen3-max")
    prompt = ChatPromptTemplate.from_messages([
        ("system", f"""å†³å®šæ˜¯ç»§ç»­ç ”ç©¶è¿˜æ˜¯å®Œæˆç­”æ¡ˆã€‚å½“å‰æ—¥æœŸï¼š{current_date}ï¼ˆ{current_year}å¹´ï¼‰

ç»§ç»­ç ”ç©¶çš„æƒ…å†µï¼š
- å­˜åœ¨éœ€è¦æ›´å¤šè¯æ®çš„å…·ä½“æœªè§£å†³é—®é¢˜
- å½“å‰å‘ç°ä¸è¶³ä»¥å½¢æˆå…¨é¢ç­”æ¡ˆ
- æ–°è¯æ®è¡¨æ˜æœ‰å¸Œæœ›çš„ç ”ç©¶æ–¹å‘

å®Œæˆç­”æ¡ˆçš„æƒ…å†µï¼š
- ä¸»è¦ç ”ç©¶é—®é¢˜å·²å¾—åˆ°å……åˆ†è§£ç­”
- æœ€è¿‘å‡ è½®æœªäº§ç”Ÿé‡è¦æ–°ä¿¡æ¯
- æ”¶é›†çš„è¯æ®è¶³ä»¥å½¢æˆå…¨é¢å›åº”
- å·²è¾¾åˆ°æœ€å¤§è½®æ¬¡

è¿”å› JSONï¼š
{{{{
    "action": "continue" æˆ– "stop",
    "reason": "è§£é‡Š",
    "confidence": "high" æˆ– "medium" æˆ– "low"
}}}}"""),
        ("human", """å½“å‰çŠ¶æ€ï¼š
è½®æ¬¡ï¼š{{round_id}}/{{max_rounds}}
æ‘˜è¦ï¼š{{summary}}
å‘ç°æ•°ï¼š{{findings_count}}
å¾…è§£å†³é—®é¢˜æ•°ï¼š{{open_questions_count}}
æœ¬è½®æ–°è¯æ®æ•°ï¼š{{new_evidence_count}}

æœ€è¿‘å‘ç°ï¼š
{{recent_findings}}

æœ€è¿‘å¾…è§£å†³é—®é¢˜ï¼š
{{recent_questions}}

åº”è¯¥ç»§ç»­ç ”ç©¶è¿˜æ˜¯å®Œæˆç­”æ¡ˆï¼Ÿ"""),
    ])
    
    chain = prompt | llm
    response = await chain.ainvoke({
        "round_id": state["round_id"],
        "max_rounds": state["max_rounds"],
        "summary": state.get("summary", "æš‚æ— "),
        "findings_count": len(state.get("findings", [])),
        "open_questions_count": len(state.get("open_questions", [])),
        "new_evidence_count": len(state.get("evidence_packs", [])),
        "recent_findings": "\n".join(f"- {f}" for f in state.get("findings", [])[-3:]),
        "recent_questions": "\n".join(f"- {q}" for q in state.get("open_questions", [])[:3]),
    })
    
    try:
        content = _extract_text_from_response(response)
        decision = json.loads(content)
    except Exception:
        # Default to continue on parse error
        decision = {"action": "continue", "reason": "è§£æé”™è¯¯ï¼Œé»˜è®¤ç»§ç»­", "confidence": "low"}
    
    next_action = decision.get("action", "continue")
    if next_action == "stop":
        return {
            "next_action": "stop",
            "stop_reason": decision.get("reason", "LLMå†³å®šåœæ­¢"),
            "trace": [{
                "round": state["round_id"],
                "stage": "check_stop",
                "timestamp": datetime.now().isoformat(),
                "message": f"åœæ­¢ï¼š{decision.get('reason', 'LLMå†³å®š')}",
                "data": decision,
            }],
        }
    else:
        return {
            "trace": [{
                "round": state["round_id"],
                "stage": "check_stop",
                "timestamp": datetime.now().isoformat(),
                "message": f"ç»§ç»­ç ”ç©¶ï¼š{decision.get('reason', 'LLMå†³å®š')}",
                "data": decision,
            }],
        }


@trace_node("write_answer")
async def write_final_answer_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆï¼ˆé‡æ„ç‰ˆï¼šæ‘˜è¦é©±åŠ¨+æŒ‰éœ€å–åŸæ–‡ï¼‰
    
    ç­–ç•¥ï¼š
    1. ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼šåªç»™ all_notes çš„æ‘˜è¦å±‚ï¼ˆkey_points + evidence_quotesï¼‰
    2. LLM å¯ä»¥è¾“å‡º <answer>...</answer> æˆ– JSON: {"type":"NEED_RAW","requests":[...]}
    3. è‹¥ NEED_RAWï¼šä» Session Cache æå–æŒ‡å®š doc_id çš„ chunksï¼Œè¡¥å……åå†è°ƒç”¨
    4. æœ€å¤šå¾ªç¯ 2-3 æ¬¡ï¼Œæœ€ç»ˆå¿…é¡»äº§å‡º <answer>
    
    è¾“å…¥: query, task_spec, summary, findings, all_notes, session_cache
    è¾“å‡º: final_answer, citations
    """
    logger.info(f"[Task {state['task_id']}] Writing final answer (Session Cache mode)")
    logger.debug(f"ğŸ“¥ Total findings: {len(state.get('findings', []))}, Notes: {len(state.get('all_notes', []))}")
    
    # æ¢å¤ SessionCacheï¼ˆä½¿ç”¨æ–°çš„ååºåˆ—åŒ–æ–¹æ³•ï¼‰
    cache_data = state.get("session_cache", {})
    cache = SessionCache.from_state_dict(cache_data) if cache_data else SessionCache()
            cache.add_doc(doc)
    
    all_notes = state.get("all_notes", [])
    if not all_notes:
        # Fallbackï¼šæ²¡æœ‰ notesï¼Œä½¿ç”¨ä¼ ç»Ÿ Milvus æ£€ç´¢ï¼ˆä¸æ¨èï¼‰
        logger.warning("No all_notes found, falling back to Milvus retrieval")
        return await _write_answer_fallback(state)
    
    llm = make_llm(model="qwen3-max")
    
    # === Phase 1: æ„å»ºæ‘˜è¦å±‚è¯æ®ï¼ˆä¸åŒ…å«åŸæ–‡ï¼‰===
    evidence_summary_lines = []
    for note in all_notes[:20]:  # é™åˆ¶æ•°é‡
        if note.get("media_type") == "image":
            evidence_summary_lines.append(
                f"[IMAGE] [{note['pack_id']}] {note['source']}:{note['ref']}\n"
                f"  Caption: {note['snippet']}\n"
                f"  URL: {note.get('image_url', '')}"
            )
        else:
            evidence_summary_lines.append(
                f"[{note['pack_id']}] {note['source']}:{note['ref']} - {note.get('title', '')}\n"
                f"  Snippet: {note['snippet']}"
            )
            
            # Key points
            for kp in note.get("key_points", [])[:3]:
                evidence_summary_lines.append(f"    * {kp}")
            
            # Evidence quotesï¼ˆå¸¦ location_hintï¼‰
            for eq in note.get("evidence_quotes", [])[:3]:
                quote = eq.get("quote", "")[:200]
                why = eq.get("why_relevant", "")[:100]
                loc = eq.get("location_hint", "")
                evidence_summary_lines.append(f"    â†’ Quote [{loc}]: {quote}\n      Why: {why}")
    
    evidence_summary = "\n".join(evidence_summary_lines)
    
    # === Phase 2: å¤šè½®å¯¹è¯ï¼ˆæœ€å¤š 3 æ¬¡ï¼‰===
    max_rounds = 3
    final_answer = ""
    raw_chunks_provided = []  # è®°å½•å·²æä¾›çš„åŸæ–‡
    
    for attempt in range(max_rounds):
        logger.info(f"Write answer attempt {attempt + 1}/{max_rounds}")
        
        # æ„å»º prompt
        if attempt == 0:
            # ç¬¬ä¸€æ¬¡ï¼šåªç»™æ‘˜è¦
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ï¼Œæ­£åœ¨æ’°å†™ç ”ç©¶æŸ¥è¯¢çš„æœ€ç»ˆç­”æ¡ˆã€‚

**å½“å‰é˜¶æ®µ**ï¼šä½ å°†çœ‹åˆ°è¯æ®çš„**æ‘˜è¦å±‚**ï¼ˆkey_points + evidence_quotesï¼‰ï¼Œæ²¡æœ‰å®Œæ•´åŸæ–‡ã€‚

**ä½ çš„ä»»åŠ¡**ï¼š
1. å¦‚æœæ‘˜è¦è¶³å¤Ÿå›ç­”é—®é¢˜ï¼šç›´æ¥è¾“å‡º <answer>...</answer>ï¼ˆå®Œæ•´ç­”æ¡ˆï¼Œä¸­æ–‡ï¼Œå¼•ç”¨ [pack_id]ï¼‰
2. å¦‚æœéœ€è¦æŸ¥çœ‹åŸæ–‡ç»†èŠ‚ï¼šè¾“å‡º JSONï¼š
   ```json
   {
     "type": "NEED_RAW",
     "requests": [
       {"pack_id": "xxx", "reason": "ä¸ºä»€ä¹ˆéœ€è¦è¿™æ®µåŸæ–‡", "location_hint": "P3"}
     ]
   }
   ```

**è¾“å‡ºè§„åˆ™**ï¼š
- å¦‚æœè¾“å‡º <answer>ï¼šå¿…é¡»åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ã€å¼•ç”¨ [pack_id]ã€å›¾ç‰‡ç”¨ ![desc](url)
- å¦‚æœè¾“å‡º NEED_RAWï¼šæœ€å¤šè¯·æ±‚ 3 ä¸ª doc çš„åŸæ–‡ç‰‡æ®µ
- ä¸è¦åŒæ—¶è¾“å‡º <answer> å’Œ NEED_RAW

**å›¾ç‰‡å¤„ç†**ï¼š
- å›¾ç‰‡è¯æ®æ ‡è®°ä¸º [IMAGE]ï¼Œä½¿ç”¨æä¾›çš„ URL åµŒå…¥ï¼š`![æè¿°](url)`
- ä¸è¦ç¼–é€  URL"""
            
            user_prompt = f"""ç ”ç©¶æŸ¥è¯¢ï¼š{state['query']}

ä»»åŠ¡è§„æ ¼ï¼š{json.dumps(state.get('task_spec', {}), ensure_ascii=False)}

ç ”ç©¶æ‘˜è¦ï¼š{state.get('summary', '')}

å…³é”®å‘ç°ï¼š
{chr(10).join(f"- {f}" for f in state.get('findings', []))}

è¯æ®æ‘˜è¦å±‚ï¼ˆKey Points + Quotesï¼‰ï¼š
{evidence_summary}

å‰©ä½™å¾…è§£å†³é—®é¢˜ï¼š
{chr(10).join(f"- {q}" for q in state.get('open_questions', [])) if state.get('open_questions') else "None"}

è¯·è¾“å‡º <answer>...</answer> æˆ– NEED_RAW JSONã€‚"""
        
        else:
            # åç»­è½®æ¬¡ï¼šè¡¥å……åŸæ–‡
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ï¼Œç»§ç»­æ’°å†™ç­”æ¡ˆã€‚

**å½“å‰é˜¶æ®µ**ï¼šä½ å·²ç»çœ‹è¿‡è¯æ®æ‘˜è¦ï¼Œç°åœ¨æä¾›äº†ä½ è¯·æ±‚çš„**åŸæ–‡ç‰‡æ®µ**ã€‚

**ä½ çš„ä»»åŠ¡**ï¼š
1. ç»“åˆåŸæ–‡ç‰‡æ®µï¼Œè¾“å‡ºå®Œæ•´ç­”æ¡ˆ <answer>...</answer>
2. å¦‚æœä»éœ€æ›´å¤šåŸæ–‡ï¼ˆæœ€å¤šå†è¯·æ±‚ 1 æ¬¡ï¼‰ï¼šè¾“å‡º NEED_RAW JSON
3. **å¿…é¡»**åœ¨æœ¬è½®æˆ–ä¸‹ä¸€è½®è¾“å‡º <answer>ï¼Œä¸èƒ½æ— é™å¾ªç¯

**è¾“å‡ºè§„åˆ™**ï¼šåŒä¸Šä¸€è½®"""
            
            user_prompt = f"""ç ”ç©¶æŸ¥è¯¢ï¼š{state['query']}

ä¹‹å‰çš„è¯æ®æ‘˜è¦ï¼š
{evidence_summary[:2000]}...

ä½ è¯·æ±‚çš„åŸæ–‡ç‰‡æ®µï¼š
{chr(10).join(f"[Doc {c['source']}] Location: {c['location']}\n{c['text'][:1000]}\n" for c in raw_chunks_provided[-3:])}

è¯·è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ <answer>...</answer> æˆ–ï¼ˆæœ€åä¸€æ¬¡æœºä¼šï¼‰NEED_RAW JSONã€‚"""
        
        response = await llm.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ])
        
        raw_resp = _extract_text_from_response(response)
        
        # æ£€æŸ¥æ˜¯å¦è¾“å‡º <answer>
        import re
        answer_match = re.search(r'<answer>(.*?)</answer>', raw_resp, re.DOTALL)
        if answer_match:
            final_answer = answer_match.group(1).strip()
            logger.info(f"Got final answer on attempt {attempt + 1}")
            break
        
        # æ£€æŸ¥æ˜¯å¦è¾“å‡º NEED_RAW
        try:
            # å°è¯•è§£æ JSON
            need_raw_match = re.search(r'\{[^}]*"type"\s*:\s*"NEED_RAW"[^}]*\}', raw_resp, re.DOTALL)
            if need_raw_match:
                need_raw = json.loads(need_raw_match.group(0))
            else:
                # å°è¯•å®Œæ•´è§£æ
                need_raw = json.loads(raw_resp)
            
            if need_raw.get("type") == "NEED_RAW":
                requests = need_raw.get("requests", [])
                logger.info(f"LLM requested {len(requests)} raw chunks")
                
                # æå–åŸæ–‡
                for req in requests[:3]:  # æœ€å¤š 3 ä¸ª
                    pack_id = req.get("pack_id")
                    location_hint = req.get("location_hint")
                    
                    if pack_id:
                        note = cache.get_note(pack_id)
                        if note and note.get("raw_pointer"):
                            doc_id = note["raw_pointer"]
                            chunks = cache.extract_chunks_for_need_raw(
                                doc_id=doc_id,
                                location_hints=[location_hint] if location_hint else None,
                                pack_ids=[pack_id],
                                max_chunks=2,
                                max_chars=2000
                            )
                            raw_chunks_provided.extend(chunks)
                
                if not raw_chunks_provided:
                    # æ²¡æœ‰æå–åˆ°åŸæ–‡ï¼Œå¼ºåˆ¶è¾“å‡ºç­”æ¡ˆ
                    logger.warning("NEED_RAW but no chunks found, forcing answer")
                    break
                
                # ç»§ç»­ä¸‹ä¸€è½®
                continue
            else:
                # ä¸æ˜¯ NEED_RAWï¼Œä¹Ÿä¸æ˜¯ <answer>ï¼Œå½“ä½œé”™è¯¯
                logger.warning(f"Unexpected LLM output format: {raw_resp[:200]}")
                break
        
        except json.JSONDecodeError:
            # ä¸æ˜¯ JSONï¼Œä¹Ÿæ²¡æœ‰ <answer> æ ‡ç­¾
            logger.warning(f"LLM output is neither <answer> nor NEED_RAW JSON: {raw_resp[:200]}")
            # Fallbackï¼šä½¿ç”¨åŸå§‹è¾“å‡º
            final_answer = raw_resp
            break
    
    # å¦‚æœå¾ªç¯ç»“æŸè¿˜æ²¡æœ‰ answerï¼Œä½¿ç”¨æœ€åä¸€æ¬¡è¾“å‡º
    if not final_answer:
        logger.warning("Failed to get <answer> after max rounds, using last LLM output")
        final_answer = raw_resp
    
    # === Phase 3: æ„å»º citationsï¼ˆæ¥è‡ª all_notesï¼‰===
    citations = []
    for note in all_notes[:20]:
        citations.append({
            "source": note["source"],
            "ref": note["ref"],
            "snippet": note["snippet"],
            "pack_id": note["pack_id"],
            "url": note.get("url"),
            "title": note.get("title"),
            "media_type": note.get("media_type", "text"),
            "image_url": note.get("image_url")
        })
    
    return {
        "final_answer": final_answer,
        "citations": citations,
        "trace": [{
            "round": state["round_id"],
            "stage": "write_final_answer",
            "timestamp": datetime.now().isoformat(),
            "message": f"Generated answer with {len(citations)} citations, {len(raw_chunks_provided)} raw chunks provided",
        }],
    }


async def _write_answer_fallback(state: ResearchState) -> ResearchStateUpdate:
    """Fallback: ä¼ ç»Ÿ Milvus æ£€ç´¢æ¨¡å¼ï¼ˆä¸æ¨èï¼‰"""
    logger.warning("Using fallback write_answer mode (Milvus retrieval)")
    embedding_provider = get_embedding_provider()
    retriever = get_retriever()
    
    query_embedding = await embedding_provider.embed(state["query"])
    all_evidence = retriever.search_evidence(
        query_embedding=query_embedding,
        top_k=20,
        task_id=state["task_id"],
        enable_rerank=False,
    )
    
    evidence_lines = []
    for ev in all_evidence:
        media_type = ev.get('media_type', 'text')
        if media_type == 'image':
            evidence_lines.append(
                f"[IMAGE] [{ev['source']}:{ev['ref']}] {ev['snippet']} (URL: {ev.get('image_url', '')})"
            )
        else:
            evidence_lines.append(f"[{ev['source']}:{ev['ref']}] {ev['snippet']}")
    
    evidence_text = "\n".join(evidence_lines)
    
    llm = make_llm(model="qwen3-max")
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", """ä½ æ˜¯ä¸€ä¸ªç ”ç©¶åŠ©æ‰‹ï¼Œæ­£åœ¨æ’°å†™ç ”ç©¶æŸ¥è¯¢çš„æœ€ç»ˆç­”æ¡ˆã€‚
åŸºäºç ”ç©¶å‘ç°å’Œè¯æ®ï¼Œæä¾›å…¨é¢çš„ç­”æ¡ˆã€‚

æŒ‡å—ï¼š
1. å›åº”åŸå§‹æŸ¥è¯¢çš„æ‰€æœ‰æ–¹é¢
2. ä½¿ç”¨ [æ¥æº:å¼•ç”¨] æ ¼å¼å¼•ç”¨æ¥æº
3. å¦‚éœ€è¦ï¼Œç”¨ç« èŠ‚æ¸…æ™°ç»„ç»‡ç­”æ¡ˆ
4. å…¨é¢ä½†ç®€æ´
5. æ³¨æ˜ä»»ä½•å±€é™æ€§æˆ–éœ€è¦è¿›ä¸€æ­¥ç ”ç©¶çš„é¢†åŸŸ
6. ä½¿ç”¨ä¸­æ–‡å›ç­”

**å›¾ç‰‡å¤„ç†ï¼š**
- å¦‚æœè¯æ®ä¸­åŒ…å«å›¾ç‰‡ï¼ˆæ ‡è®°ä¸º [IMAGE]ï¼‰ï¼Œè¯·ä½¿ç”¨ markdown æ ¼å¼åµŒå…¥ï¼š`![æè¿°](url)`
- **å¿…é¡»ä¸”åªèƒ½**ä½¿ç”¨è¯æ®ä¸­æä¾›çš„ URLï¼Œä¸è¦ç¼–é€ é“¾æ¥

**é‡è¦**ï¼šä½ å¿…é¡»æŠŠæœ€ç»ˆçš„ç»“è®ºåŒ…è£¹åœ¨ <answer> æ ‡ç­¾ä¸­ã€‚

ç¤ºä¾‹æ ¼å¼ï¼š
<answer>
[è¿™é‡Œæ˜¯æœ€ç»ˆç­”æ¡ˆï¼ŒåŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯ã€å¼•ç”¨å’Œå›¾ç‰‡]
![å›¾è¡¨è¯´æ˜](https://example.com/image.png)
</answer>"""),
        ("human", """ç ”ç©¶æŸ¥è¯¢ï¼š{query}

ä»»åŠ¡è§„æ ¼ï¼š{task_spec}

ç ”ç©¶æ‘˜è¦ï¼š{summary}

å…³é”®å‘ç°ï¼š
{findings}

è¯æ®ï¼š
{evidence}

å‰©ä½™å¾…è§£å†³é—®é¢˜ï¼š
{open_questions}

è¯·æä¾›æœ€ç»ˆçš„å…¨é¢ç­”æ¡ˆï¼ˆä¸­æ–‡ï¼‰ã€‚"""),
    ])
    
    chain = prompt | llm
    response = await chain.ainvoke({
        "query": state["query"],
        "task_spec": json.dumps(state.get("task_spec", {})),
        "summary": state.get("summary", ""),
        "findings": "\n".join(f"- {f}" for f in state.get("findings", [])),
        "evidence": evidence_text,
        "open_questions": "\n".join(f"- {q}" for q in state.get("open_questions", [])) if state.get("open_questions") else "None",
    })
    
    citations = [
        {
            "source": ev["source"],
            "ref": ev["ref"],
            "snippet": ev["snippet"],
            "metadata": ev.get("metadata", {}),
        }
        for ev in all_evidence
    ]
    
    raw_response = _extract_text_from_response(response)
    import re
    answer_match = re.search(r'<answer>(.*?)</answer>', raw_response, re.DOTALL)
    if answer_match:
        final_answer = answer_match.group(1).strip()
    else:
        final_answer = raw_response
        logger.warning("No <answer> tag found in LLM response, using full response")
    
    return {
        "final_answer": final_answer,
        "citations": citations,
        "trace": [{
            "round": state["round_id"],
            "stage": "write_final_answer",
            "timestamp": datetime.now().isoformat(),
            "message": f"Generated answer with {len(citations)} citations (fallback mode)",
        }],
    }


# ============================================================================
# Conditional Edges (è·¯ç”±é€»è¾‘)
# ============================================================================

def should_continue(state: ResearchState) -> str:
    """
    å†³å®šå·¥ä½œæµä¸‹ä¸€æ­¥ï¼šç»§ç»­è¿­ä»£ vs è¾“å‡ºç­”æ¡ˆ
    
    è¿”å›å€¼å¯¹åº” graph edge çš„ target node name
    """
    next_action = state.get("next_action", "continue")
    
    if next_action == "answer":
        return "write_answer"
    elif next_action == "stop":
        return "write_answer"
    else:
        # å¢åŠ è½®æ¬¡ï¼Œç»§ç»­ä¸‹ä¸€è½®
        return "next_round"


def increment_round(state: ResearchState) -> ResearchStateUpdate:
    """Helper node: å¢åŠ è½®æ¬¡è®¡æ•°"""
    return {"round_id": state["round_id"] + 1}


@trace_node("persist_evidence")
async def persist_evidence_node(state: ResearchState) -> ResearchStateUpdate:
    """
    Node: æŒä¹…åŒ–è¯æ®åˆ° Milvusï¼ˆä»»åŠ¡ç»“æŸåï¼‰
    
    ç­–ç•¥ï¼š
    1. åªå†™"å¯æ£€ç´¢çš„ç¬”è®°æ–‡æœ¬"ï¼ševidence_quotes.quote + key_points
    2. æŒ‰ chunk/quote ç²’åº¦å†™å…¥ï¼ˆä¸æ˜¯æ•´ç¯‡ docï¼‰
    3. metadata åŒºåˆ† origin=local/web
    4. å»é‡ï¼šsource+ref+hash_text(quote)
    
    è¾“å…¥: persist_queue, all_notes, session_cache
    è¾“å‡º: æ— ï¼ˆå‰¯ä½œç”¨ï¼šå†™å…¥ Milvusï¼‰
    """
    logger.info(f"[Task {state['task_id']}] Persisting evidence to Milvus")
    
    persist_queue = state.get("persist_queue", [])
    all_notes = state.get("all_notes", [])
    
    if not persist_queue:
        logger.info("No evidence to persist (empty queue)")
        return {
            "trace": [{
                "round": state["round_id"],
                "stage": "persist_evidence",
                "timestamp": datetime.now().isoformat(),
                "message": "No evidence to persist",
            }]
        }
    
    embedding_provider = get_embedding_provider()
    writer = get_writer()
    
    # æ¢å¤ session_cache è·å– origin ä¿¡æ¯
    cache = SessionCache()
    if state.get("session_cache"):
        cached_data = state["session_cache"]
        for doc_id, doc_dict in cached_data.get("docs", {}).items():
            # ç®€åŒ–ï¼šåªè®°å½• origin
            cache.docs[doc_id] = type('Doc', (), {'origin': doc_dict.get('origin', 'web'), 'doc_id': doc_id})()
    
    chunks_to_write = []
    written_ids = set()
    
    for pack_id in persist_queue:
        # æ‰¾åˆ°å¯¹åº”çš„ note
        note = next((n for n in all_notes if n['pack_id'] == pack_id), None)
        if not note:
            continue
        
        # è·å– origin
        raw_pointer = note.get("raw_pointer")
        origin = "web"
        if raw_pointer and raw_pointer in cache.docs:
            origin = getattr(cache.docs[raw_pointer], 'origin', 'web')
        
        # ç­–ç•¥1: å†™å…¥æ¯ä¸ª evidence_quote ä½œä¸ºç‹¬ç«‹ chunk
        for idx, eq in enumerate(note.get("evidence_quotes", [])[:5]):
            quote = eq.get("quote", "")
            if not quote or len(quote) < 20:
                continue
            
            # å»é‡
            chunk_id = generate_evidence_id(note["source"], note["ref"], hash_text(quote))
            if chunk_id in written_ids:
                continue
            written_ids.add(chunk_id)
            
            # ç”Ÿæˆ embedding
            embedding = await embedding_provider.embed(quote)
            
            # snippet = quote[:300]
            snippet = eq.get("why_relevant", "")[:200] or quote[:200]
            
            chunks_to_write.append({
                "id": chunk_id,
                "embedding": embedding,
                "text": quote,  # å®Œæ•´ quote
                "source": note["source"],
                "ref": note["ref"],
                "snippet": snippet,
                "task_id": state["task_id"],
                "round_id": state["round_id"],
                "metadata": {
                    "title": note.get("title", ""),
                    "url": note.get("url", ""),
                    "pack_id": pack_id,
                    "location_hint": eq.get("location_hint", ""),
                    "why_relevant": eq.get("why_relevant", ""),
                    "origin": origin,  # åŒºåˆ† local/web
                    "media_type": note.get("media_type", "text"),
                    "image_url": note.get("image_url"),
                    "target_question": note.get("target_question", ""),
                    "chunk_type": "evidence_quote"  # æ ‡è®°ä¸ºè¯æ®å¼•ç”¨
                },
                "created_at": int(datetime.now().timestamp())
            })
        
        # ç­–ç•¥2: å†™å…¥ key_points æ‹¼æ¥ï¼ˆä½œä¸ºèƒŒæ™¯å—ï¼Œå¯é€‰ï¼‰
        key_points_text = " | ".join(note.get("key_points", [])[:5])
        if key_points_text and len(key_points_text) > 50:
            chunk_id = generate_evidence_id(note["source"], note["ref"], hash_text(key_points_text))
            if chunk_id not in written_ids:
                written_ids.add(chunk_id)
                
                embedding = await embedding_provider.embed(key_points_text)
                
                chunks_to_write.append({
                    "id": chunk_id,
                    "embedding": embedding,
                    "text": key_points_text,
                    "source": note["source"],
                    "ref": note["ref"],
                    "snippet": key_points_text[:300],
                    "task_id": state["task_id"],
                    "round_id": state["round_id"],
                    "metadata": {
                        "title": note.get("title", ""),
                        "url": note.get("url", ""),
                        "pack_id": pack_id,
                        "origin": origin,
                        "media_type": note.get("media_type", "text"),
                        "image_url": note.get("image_url"),
                        "target_question": note.get("target_question", ""),
                        "chunk_type": "key_points"  # æ ‡è®°ä¸ºå…³é”®ç‚¹
                    },
                    "created_at": int(datetime.now().timestamp())
                })
    
    # æ‰¹é‡å†™å…¥
    if chunks_to_write:
        logger.info(f"Writing {len(chunks_to_write)} chunks to Milvus (originåˆ†å¸ƒ: {sum(1 for c in chunks_to_write if c['metadata'].get('origin') == 'local')} local, {sum(1 for c in chunks_to_write if c['metadata'].get('origin') == 'web')} web)")
        writer.write_evidence_batch(chunks_to_write)
    
    return {
        "trace": [{
            "round": state["round_id"],
            "stage": "persist_evidence",
            "timestamp": datetime.now().isoformat(),
            "message": f"Persisted {len(chunks_to_write)} chunks to Milvus",
            "data": {
                "total_chunks": len(chunks_to_write),
                "local_chunks": sum(1 for c in chunks_to_write if c['metadata'].get('origin') == 'local'),
                "web_chunks": sum(1 for c in chunks_to_write if c['metadata'].get('origin') == 'web')
            }
        }]
    }


# ============================================================================
# Build StateGraph
# ============================================================================

def create_research_graph() -> StateGraph:
    """
    æ„å»º DeepResearch LangGraph å·¥ä½œæµï¼ˆé‡æ„ç‰ˆï¼‰
    
    æ–°æµç¨‹ç»“æ„ï¼š
    START -> parse_task -> build_workspace -> tool_loop -> materialize_round_cache
                                                                    |
                                                                    v
                                                (skip normalize_evidenceï¼Œå·²åœ¨materializeå®Œæˆ)
                                                                    |
                                                                    v
                                                      update_report -> check_stop
                                                                    |
                                                                    v
                                    [continue] -> increment_round -> build_workspace (å¾ªç¯)
                                    [stop/answer] -> write_answer -> persist_evidence -> END
    
    å…³é”®æ”¹åŠ¨ï¼š
    1. tool_loop ä¹‹åå¿…é¡»ç»è¿‡ materialize_round_cacheï¼ˆå³ä½¿next_action=answerï¼‰
    2. materialize å®Œæˆ hitâ†’docâ†’note å‡çº§å’Œåˆ†å—
    3. write_answer ä½¿ç”¨ Session Cache è€Œé Milvus æ£€ç´¢
    4. persist_evidence åœ¨æœ€åæ‰§è¡Œï¼ˆä»»åŠ¡ç»“æŸåå…¥åº“ï¼‰
    """
    workflow = StateGraph(ResearchState)
    
    # Add nodes
    workflow.add_node("parse_task", parse_task_node)  # type: ignore
    workflow.add_node("build_workspace", build_workspace_node)  # type: ignore
    workflow.add_node("tool_loop", tool_loop_node)  # type: ignore
    workflow.add_node("materialize_round_cache", materialize_round_cache_node)  # type: ignore [NEW]
    workflow.add_node("normalize_evidence", normalize_evidence_node)  # type: ignore [DEPRECATED, ä¿ç•™å…¼å®¹]
    workflow.add_node("update_report", update_report_node)  # type: ignore
    workflow.add_node("check_stop", check_stop_node)  # type: ignore
    workflow.add_node("write_answer", write_final_answer_node)  # type: ignore
    workflow.add_node("persist_evidence", persist_evidence_node)  # type: ignore [NEW]
    workflow.add_node("increment_round", increment_round)  # type: ignore

    
    # Set entry point
    workflow.set_entry_point("parse_task")
    
    # Add edges
    workflow.add_edge("parse_task", "build_workspace")
    workflow.add_edge("build_workspace", "tool_loop")
    
    # === å…³é”®æ”¹åŠ¨ï¼štool_loop åå¿…é¡»å…ˆ materializeï¼ˆæ— è®º next_actionï¼‰===
    workflow.add_edge("tool_loop", "materialize_round_cache")
    
    # materialize åæ ¹æ® next_action åˆ†æ”¯
    workflow.add_conditional_edges(
        "materialize_round_cache",
        lambda s: "write_answer" if s.get("next_action") in ("answer", "stop") else "update_report",
        {"write_answer": "write_answer", "update_report": "update_report"}
    )
    
    # ç»§ç»­ç ”ç©¶è·¯å¾„
    workflow.add_edge("update_report", "check_stop")
    workflow.add_conditional_edges(
        "check_stop",
        lambda state: "write_answer" if state.get("next_action") == "stop" else "increment_round",
        {
            "write_answer": "write_answer",
            "increment_round": "increment_round",
        }
    )
    workflow.add_edge("increment_round", "build_workspace")  # Loop back
    
    # === å…³é”®æ”¹åŠ¨ï¼šwrite_answer åå¿…é¡» persist_evidence å† END ===
    workflow.add_edge("write_answer", "persist_evidence")
    workflow.add_edge("persist_evidence", END)
    
    return workflow
    
    return workflow


# ============================================================================
# Service Interface
# ============================================================================

class DeepResearchLangGraphService:
    """
    DeepResearch Service using LangGraph
    
    ä½¿ç”¨æ–¹å¼:
        service = DeepResearchLangGraphService()
        async for event in service.run_research(query="What is RAG?"):
            print(event)
    """
    
    def __init__(self, checkpointer=None):
        """
        Initialize service
        
        Args:
            checkpointer: LangGraph checkpointer for state persistence
                         (e.g., MemorySaver, SqliteSaver, PostgresSaver)
        """
        self.workflow = create_research_graph()
        self.checkpointer = checkpointer or MemorySaver()
        self.app = self.workflow.compile(checkpointer=self.checkpointer)
        
        # åˆå§‹åŒ–è¿½è¸ªé…ç½®
        self.tracing_config = get_tracing_config()
        logger.info(f"ğŸ” è¿½è¸ªé…ç½®: LangSmith={'âœ…' if self.tracing_config.langsmith_enabled else 'âŒ'}, æœ¬åœ°è¿½è¸ª={'âœ…' if self.tracing_config.local_trace_enabled else 'âŒ'}")
        
        # æ‰“å°å·¥ä½œæµå›¾ï¼ˆè°ƒè¯•æ¨¡å¼ï¼‰
        if os.getenv("DEBUG", "false").lower() == "true":
            print_workflow_diagram()
    
    async def run_research(
        self,
        query: str,
        max_rounds: int = 5,
        max_papers: int = 10,
        max_repos: int = 5,
        top_k: int = 10,
        config: Optional[Dict[str, Any]] = None,
    )-> AsyncIterator[NodeCompleteEvent]:
        """
        è¿è¡Œç ”ç©¶ä»»åŠ¡ï¼ˆæµå¼è¾“å‡ºï¼‰
        
        Args:
            query: ç ”ç©¶é—®é¢˜
            max_rounds: æœ€å¤§è½®æ•°
            max_papers: æ¯è½®æœ€å¤šè®ºæ–‡æ•°
            max_repos: æ¯è½®æœ€å¤šä»“åº“æ•°
            top_k: æ£€ç´¢ top-k
            config: LangGraph è¿è¡Œé…ç½®ï¼ˆå¯åŒ…å« thread_idï¼‰
        
        Yields:
            Dict: çŠ¶æ€æ›´æ–°äº‹ä»¶
        """
        task_id = str(uuid.uuid4())
        
        logger.info(f"ğŸš€ å¼€å§‹ç ”ç©¶ä»»åŠ¡: {task_id}")
        logger.info(f"ğŸ“‹ Query: {query}")
        logger.info(f"âš™ï¸ Config: max_rounds={max_rounds}, max_papers={max_papers}, max_repos={max_repos}")
        
        # Initial state
        initial_state = {
            "task_id": task_id,
            "query": query,
            "round_id": 1,
            "task_spec": {},
            "summary": "",
            "findings": [],
            "open_questions": [],
            "evidence_ids": [],
            # Session Cache (æ–°å¢)
            "session_cache": {},
            "round_doc_ids": [],
            "all_notes": [],
            "round_notes": [],
            "persist_queue": [],
            # Evidence packs (ä¿ç•™å…¼å®¹)
            "evidence_packs": [],
            "new_evidence_packs": [],
            "workspace": "",
            "tool_queries": [],
            "tool_results": [],
            "next_action": "continue",
            "stop_reason": None,
            "messages": [],
            "final_answer": "",
            "citations": [],
            "max_rounds": max_rounds,
            "max_papers": max_papers,
            "max_repos": max_repos,
            "top_k": top_k,
            # Tool loop / workspace limits for progressive disclosure
            "max_tool_steps_per_round": 3,
            "max_workspace_chars": 8000,
            "max_packs_per_step": 3,
            "max_total_packs_per_round": 20,
            "trace": [],
        }
        
        # ä½¿ç”¨è¿½è¸ªé…ç½®ç”Ÿæˆ configï¼ˆåŒ…å« LangSmith å…ƒæ•°æ®ï¼‰
        if config is None:
            config = self.tracing_config.get_langchain_config(task_id)
        else:
            # åˆå¹¶ç”¨æˆ·æä¾›çš„ config å’Œè¿½è¸ªé…ç½®
            trace_config = self.tracing_config.get_langchain_config(task_id)
            config.setdefault("metadata", {}).update(trace_config.get("metadata", {}))
        
        logger.info(f"ğŸ” è¿½è¸ªé…ç½®: {config.get('metadata', {})}")
        
        # Run graph with streaming
        async for event in self.app.astream(initial_state, cast(RunnableConfig, config)):
            for node_name, output_state in event.items():
                yield {
                "type": "node_complete",
                "node": str(node_name),
                "state": cast(Dict[str, Any], output_state),  # âœ… ä¸å†è¦æ±‚ ResearchState
                "task_id": task_id,
            }
    
    async def get_state(self, task_id: str) -> Optional[ResearchState]:
        config = {"configurable": {"thread_id": task_id}}
        try:
            snap = await self.app.aget_state(cast(RunnableConfig, config))
            if not snap:
                return None
            return cast(ResearchState, snap.values)  # âœ… è®© Pylance é€šè¿‡
        except Exception:
            return None



# ============================================================================
# Singleton
# ============================================================================

_service: Optional[DeepResearchLangGraphService] = None


def get_deepresearch_langgraph_service() -> DeepResearchLangGraphService:
    """Get LangGraph service instance"""
    global _service
    if _service is None:
        _service = DeepResearchLangGraphService()
    return _service


# ============================================================================
# Example Usage
# ============================================================================

async def main():
    """ç¤ºä¾‹ï¼šè¿è¡Œ LangGraph ç ”ç©¶ä»»åŠ¡"""
    service = get_deepresearch_langgraph_service()
    
    async for event in service.run_research(
        query="è°ƒç ”ä¸€ä¸‹æ¨èç³»ç»Ÿçš„æœ€æ–°è¿›å±•",
        max_rounds=3,
    ):
        node = event.get("node")
        state = event.get("state", {})
        
        print(f"\n{'='*60}")
        print(f"Node: {node}")
        
        if node == "tool_loop":
            print(f"Next Action: {state.get('next_action')}")
            print(f"Tool Results: {len(state.get('tool_results', []))} items")
            print(f"New Packs: {len(state.get('evidence_packs', []))} packs")
        
        elif node == "update_report":
            print(f"Summary: {state.get('summary', '')[:200]}...")
            print(f"Findings: {len(state.get('findings', []))} total")
            print(f"Open Questions: {state.get('open_questions')}")
        
        elif node == "write_answer":
            print(f"Final Answer:\n{state.get('final_answer', '')[:500]}...")
            print(f"Citations: {len(state.get('citations', []))}")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
