"""
LLM Tools - Language model interface
Supports: dummy (for testing), openai, ollama
"""
import logging
from typing import List, Dict, Any, Optional, AsyncGenerator
from abc import ABC, abstractmethod
import httpx
import json

from ..config import get_settings

logger = logging.getLogger(__name__)


class LLMProvider(ABC):
    """Abstract base class for LLM providers"""
    
    @abstractmethod
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Generate completion for a prompt"""
        pass
    
    @abstractmethod
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Chat completion with message history"""
        pass


class DummyLLMProvider(LLMProvider):
    """
    Dummy LLM provider for testing
    Returns mock responses that simulate real LLM behavior
    """
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Generate a mock response"""
        # Analyze prompt to generate contextual response
        prompt_lower = prompt.lower()
        
        if "summarize" in prompt_lower or "summary" in prompt_lower:
            return self._mock_summary(prompt)
        elif "question" in prompt_lower or "answer" in prompt_lower or "?" in prompt:
            return self._mock_answer(prompt)
        elif "research" in prompt_lower or "analyze" in prompt_lower:
            return self._mock_research(prompt)
        elif "extract" in prompt_lower:
            return self._mock_extraction(prompt)
        else:
            return self._mock_general(prompt)
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Chat completion with mock response"""
        # Get the last user message
        last_user_msg = ""
        for msg in reversed(messages):
            if msg.get("role") == "user":
                last_user_msg = msg.get("content", "")
                break
        
        return await self.generate(last_user_msg)
    
    def _mock_summary(self, prompt: str) -> str:
        return """## Summary

Based on the provided content, here are the key points:

1. **Main Topic**: The document discusses important concepts related to the subject matter.
2. **Key Findings**: Several significant findings were identified in the analysis.
3. **Methodology**: The approach used follows established research practices.
4. **Conclusions**: The findings support the main hypothesis with strong evidence.

This summary was generated by the dummy LLM provider for testing purposes."""

    def _mock_answer(self, prompt: str) -> str:
        return """Based on the available information, here is the answer:

The question relates to the core concepts discussed in the provided context. Key considerations include:

1. The primary factors influencing the outcome
2. Related research findings that support this conclusion
3. Potential implications for future work

**Note**: This is a mock response from the dummy LLM provider. Replace with a real LLM for production use."""

    def _mock_research(self, prompt: str) -> str:
        return """## Research Analysis

### Overview
This analysis examines the topic based on available evidence and research.

### Key Findings
1. **Finding 1**: Initial analysis reveals important patterns.
2. **Finding 2**: Cross-referencing with existing literature confirms trends.
3. **Finding 3**: Novel insights emerge from the data synthesis.

### Open Questions
- What are the long-term implications?
- How do these findings generalize to other domains?
- What additional data would strengthen the conclusions?

### Recommendations
Further investigation is recommended to validate these preliminary findings.

*Generated by dummy LLM provider for testing.*"""

    def _mock_extraction(self, prompt: str) -> str:
        return """## Extracted Information

- **Topic**: Research and Analysis
- **Keywords**: data, analysis, findings, methodology
- **Key Entities**: Research Team, Dataset, Framework
- **Summary**: Extracted key information from the provided content.

*Dummy extraction result for testing purposes.*"""

    def _mock_general(self, prompt: str) -> str:
        return f"""I've processed your request regarding: "{prompt[:100]}..."

Here's my response based on the available information:

This is a mock response from the dummy LLM provider. In production, this would be replaced with actual LLM-generated content that directly addresses your query.

Key points to consider:
1. The context provided is relevant to the question
2. Multiple perspectives should be considered
3. Further research may be beneficial

Please configure a real LLM provider (OpenAI, Ollama, etc.) for production use."""


class OpenAILLMProvider(LLMProvider):
    """OpenAI LLM provider"""
    
    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4",
        base_url: Optional[str] = None,
    ):
        self.api_key = api_key
        self.model = model
        self.base_url = base_url or "https://api.openai.com/v1"
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Generate completion using OpenAI API"""
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        return await self.chat(messages, temperature, max_tokens)
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Chat completion using OpenAI API"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": self.model,
                        "messages": messages,
                        "temperature": temperature,
                        "max_tokens": max_tokens,
                    },
                    timeout=120.0,
                )
                response.raise_for_status()
                data = response.json()
                return data["choices"][0]["message"]["content"]
        except httpx.HTTPStatusError as e:
            # 打印详细的错误响应
            error_detail = ""
            try:
                error_detail = e.response.text
            except:
                pass
            logger.error(f"OpenAI LLM HTTP error: {e}")
            logger.error(f"Response body: {error_detail}")
            # Fallback to dummy
            dummy = DummyLLMProvider()
            return await dummy.chat(messages, temperature, max_tokens)
        except Exception as e:
            logger.error(f"OpenAI LLM error: {e}")
            # Fallback to dummy
            dummy = DummyLLMProvider()
            return await dummy.chat(messages, temperature, max_tokens)


class OllamaLLMProvider(LLMProvider):
    """Ollama LLM provider"""
    
    def __init__(
        self,
        model: str = "llama2",
        base_url: str = "http://localhost:11434",
    ):
        self.model = model
        self.base_url = base_url
    
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Generate completion using Ollama API"""
        try:
            full_prompt = f"{system_prompt}\n\n{prompt}" if system_prompt else prompt
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": full_prompt,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "num_predict": max_tokens,
                        },
                    },
                    timeout=120.0,
                )
                response.raise_for_status()
                data = response.json()
                return data["response"]
        except Exception as e:
            logger.error(f"Ollama LLM error: {e}")
            dummy = DummyLLMProvider()
            return await dummy.generate(prompt, system_prompt, temperature, max_tokens)
    
    async def chat(
        self,
        messages: List[Dict[str, str]],
        temperature: float = 0.7,
        max_tokens: int = 2048,
    ) -> str:
        """Chat completion using Ollama API"""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.base_url}/api/chat",
                    json={
                        "model": self.model,
                        "messages": messages,
                        "stream": False,
                        "options": {
                            "temperature": temperature,
                            "num_predict": max_tokens,
                        },
                    },
                    timeout=120.0,
                )
                response.raise_for_status()
                data = response.json()
                return data["message"]["content"]
        except Exception as e:
            logger.error(f"Ollama chat error: {e}")
            dummy = DummyLLMProvider()
            return await dummy.chat(messages, temperature, max_tokens)


def get_llm_provider() -> LLMProvider:
    """Get the configured LLM provider"""
    settings = get_settings()
    provider = settings.llm_provider.lower()
    
    if provider == "openai":
        return OpenAILLMProvider(
            api_key=settings.llm_api_key,
            model=settings.llm_model,
            base_url=settings.llm_base_url if settings.llm_base_url else None,
        )
    elif provider == "ollama":
        return OllamaLLMProvider(
            model=settings.llm_model,
            base_url=settings.llm_base_url if settings.llm_base_url else "http://localhost:11434",
        )
    else:
        # Default to dummy provider
        return DummyLLMProvider()
