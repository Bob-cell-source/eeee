"""
å·¥ä½œæµè¿½è¸ªå·¥å…· - æ”¯æŒ LangSmith å’Œæœ¬åœ°æ—¥å¿—
"""
import os
import logging
import json
from typing import Any, Dict, Optional, Callable
from datetime import datetime
from functools import wraps
import asyncio

logger = logging.getLogger(__name__)


class TracingConfig:
    """è¿½è¸ªé…ç½®ç®¡ç†"""
    
    def __init__(self):
        self.langsmith_enabled = False
        self.langsmith_project = None
        self.local_trace_enabled = True
        self.trace_file = None
        self._setup()
    
    def _setup(self):
        """åˆå§‹åŒ–è¿½è¸ªé…ç½®"""
        # å°è¯•å¯ç”¨ LangSmith
        langchain_api_key = os.getenv("LANGCHAIN_API_KEY")
        langsmith_tracing = os.getenv("LANGCHAIN_TRACING_V2", "false").lower() == "true"
        
        if langchain_api_key and langsmith_tracing:
            try:
                # éªŒè¯ LangSmith å¯ç”¨æ€§
                import langsmith
                self.langsmith_enabled = True
                self.langsmith_project = os.getenv("LANGCHAIN_PROJECT", "deepresearch")
                logger.info(f"âœ… LangSmith è¿½è¸ªå·²å¯ç”¨ - é¡¹ç›®: {self.langsmith_project}")
            except ImportError:
                logger.warning("âš ï¸ LangSmith ä¸å¯ç”¨ (pip install langsmith)ï¼Œä½¿ç”¨æœ¬åœ°è¿½è¸ª")
                self.langsmith_enabled = False
        else:
            logger.info("â„¹ï¸ LangSmith æœªé…ç½®ï¼Œä½¿ç”¨æœ¬åœ°è¿½è¸ª")
        
        # é…ç½®æœ¬åœ°è¿½è¸ªæ–‡ä»¶
        trace_dir = os.getenv("TRACE_DIR", "logs/traces")
        os.makedirs(trace_dir, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.trace_file = os.path.join(trace_dir, f"trace_{timestamp}.jsonl")
        logger.info(f"ğŸ“ æœ¬åœ°è¿½è¸ªæ–‡ä»¶: {self.trace_file}")
    
    def get_langchain_config(self, task_id: str) -> Dict[str, Any]:
        """è·å– LangChain è¿è¡Œé…ç½®ï¼ˆåŒ…å«è¿½è¸ªå…ƒæ•°æ®ï¼‰"""
        config = {
            "configurable": {"thread_id": task_id},
            "metadata": {
                "task_id": task_id,
                "timestamp": datetime.now().isoformat(),
            }
        }
        
        if self.langsmith_enabled:
            assert self.langsmith_project is not None, "langsmith_project is None"
            config["metadata"]["langsmith_project"] = self.langsmith_project

        
        return config


# å…¨å±€è¿½è¸ªé…ç½®å®ä¾‹
_tracing_config: Optional[TracingConfig] = None


def get_tracing_config() -> TracingConfig:
    """è·å–è¿½è¸ªé…ç½®å•ä¾‹"""
    global _tracing_config
    if _tracing_config is None:
        _tracing_config = TracingConfig()
    return _tracing_config


class LocalTracer:
    """æœ¬åœ°è¿½è¸ªè®°å½•å™¨"""
    
    def __init__(self, trace_file: str):
        self.trace_file = trace_file
        self.traces = []
    
    def log_node_execution(
        self,
        task_id: str,
        node_name: str,
        input_state: Dict[str, Any],
        output_state: Dict[str, Any],
        duration_ms: float,
        error: Optional[str] = None
    ):
        """è®°å½•èŠ‚ç‚¹æ‰§è¡Œ"""
        trace_entry = {
            "timestamp": datetime.now().isoformat(),
            "task_id": task_id,
            "node": node_name,
            "duration_ms": duration_ms,
            "input_summary": self._summarize_state(input_state),
            "output_summary": self._summarize_state(output_state),
            "error": error,
        }
        
        # å†™å…¥æ–‡ä»¶
        try:
            with open(self.trace_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(trace_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"å†™å…¥è¿½è¸ªæ–‡ä»¶å¤±è´¥: {e}")
        
        self.traces.append(trace_entry)
    
    def log_llm_call(
        self,
        task_id: str,
        node_name: str,
        model: str,
        prompt_summary: str,
        response_summary: str,
        duration_ms: float,
        token_usage: Optional[Dict[str, int]] = None
    ):
        """è®°å½• LLM è°ƒç”¨"""
        trace_entry = {
            "timestamp": datetime.now().isoformat(),
            "task_id": task_id,
            "type": "llm_call",
            "node": node_name,
            "model": model,
            "prompt_summary": prompt_summary[:500],
            "response_summary": response_summary[:500],
            "duration_ms": duration_ms,
            "token_usage": token_usage,
        }
        
        try:
            with open(self.trace_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(trace_entry, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.error(f"å†™å…¥è¿½è¸ªæ–‡ä»¶å¤±è´¥: {e}")
    
    def _summarize_state(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """ç²¾ç®€çŠ¶æ€ç”¨äºè¿½è¸ª"""
        summary = {}
        
        # åªä¿ç•™å…³é”®å­—æ®µçš„æ‘˜è¦
        key_fields = ["task_id", "query", "round_id", "next_action", "stop_reason"]
        for field in key_fields:
            if field in state:
                summary[field] = state[field]
        
        # è®¡æ•°å‹å­—æ®µ
        if "evidence_packs" in state:
            summary["evidence_packs_count"] = len(state.get("evidence_packs", []))
        if "tool_results" in state:
            summary["tool_results_count"] = len(state.get("tool_results", []))
        if "findings" in state:
            summary["findings_count"] = len(state.get("findings", []))
        if "open_questions" in state:
            summary["open_questions_count"] = len(state.get("open_questions", []))
        
        # æ–‡æœ¬æ‘˜è¦
        if "summary" in state and state["summary"]:
            summary["summary_preview"] = state["summary"][:200]
        if "workspace" in state and state["workspace"]:
            summary["workspace_length"] = len(state["workspace"])
        
        return summary
    
    def get_traces(self) -> list:
        """è·å–æ‰€æœ‰è¿½è¸ªè®°å½•"""
        return self.traces


def trace_node(node_name: str):
    """èŠ‚ç‚¹æ‰§è¡Œè¿½è¸ªè£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(state: Dict[str, Any], *args, **kwargs):
            config = get_tracing_config()
            task_id = state.get("task_id", "unknown")
            
            # è®°å½•è¾“å…¥
            logger.info(f"ğŸš€ [{node_name}] å¼€å§‹æ‰§è¡Œ - Task: {task_id}, Round: {state.get('round_id', 0)}")
            logger.debug(f"ğŸ“¥ [{node_name}] è¾“å…¥çŠ¶æ€æ‘˜è¦: {_log_state_summary(state)}")
            
            start_time = asyncio.get_event_loop().time()
            error = None
            output_state = {}
            
            try:
                # æ‰§è¡ŒèŠ‚ç‚¹
                output_state = await func(state, *args, **kwargs)
                
                # è®°å½•è¾“å‡º
                logger.info(f"âœ… [{node_name}] æ‰§è¡Œå®Œæˆ")
                logger.debug(f"ğŸ“¤ [{node_name}] è¾“å‡ºçŠ¶æ€æ‘˜è¦: {_log_state_summary(output_state)}")
                
            except Exception as e:
                error = str(e)
                logger.error(f"âŒ [{node_name}] æ‰§è¡Œå¤±è´¥: {error}", exc_info=True)
                raise
            
            finally:
                # è®¡ç®—æ‰§è¡Œæ—¶é—´
                duration_ms = (asyncio.get_event_loop().time() - start_time) * 1000
                
                # æœ¬åœ°è¿½è¸ª
                if config.local_trace_enabled and config.trace_file:
                    tracer = LocalTracer(config.trace_file)
                    tracer.log_node_execution(
                        task_id=task_id,
                        node_name=node_name,
                        input_state=state,
                        output_state=output_state,
                        duration_ms=duration_ms,
                        error=error
                    )
            
            return output_state
        
        return wrapper
    return decorator


def _log_state_summary(state: Dict[str, Any]) -> str:
    """ç”ŸæˆçŠ¶æ€æ‘˜è¦æ—¥å¿—"""
    summary_parts = []
    
    if "query" in state:
        summary_parts.append(f"query='{state['query'][:50]}...'")
    if "round_id" in state:
        summary_parts.append(f"round={state['round_id']}")
    if "next_action" in state:
        summary_parts.append(f"action={state['next_action']}")
    if "evidence_packs" in state:
        summary_parts.append(f"packs={len(state.get('evidence_packs', []))}")
    if "tool_results" in state:
        summary_parts.append(f"tools={len(state.get('tool_results', []))}")
    if "findings" in state:
        summary_parts.append(f"findings={len(state.get('findings', []))}")
    
    return ", ".join(summary_parts) if summary_parts else "empty"


def trace_llm_call(node_name: str, model: str):
    """LLM è°ƒç”¨è¿½è¸ªè£…é¥°å™¨"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            config = get_tracing_config()
            
            # æå– task_idï¼ˆå¦‚æœåœ¨å‚æ•°ä¸­ï¼‰
            task_id = "unknown"
            if args and isinstance(args[0], dict):
                task_id = args[0].get("task_id", "unknown")
            
            logger.info(f"ğŸ¤– [{node_name}] LLM è°ƒç”¨å¼€å§‹ - Model: {model}")
            
            start_time = asyncio.get_event_loop().time()
            
            try:
                result = await func(*args, **kwargs)
                
                duration_ms = (asyncio.get_event_loop().time() - start_time) * 1000
                logger.info(f"âœ… [{node_name}] LLM è°ƒç”¨å®Œæˆ - {duration_ms:.0f}ms")
                
                # æœ¬åœ°è¿½è¸ª
                if config.local_trace_enabled and config.trace_file:
                    tracer = LocalTracer(config.trace_file)
                    
                    # æå– prompt å’Œ response æ‘˜è¦
                    prompt_summary = str(args)[:200] if args else ""
                    response_summary = str(result)[:200] if result else ""
                    
                    tracer.log_llm_call(
                        task_id=task_id,
                        node_name=node_name,
                        model=model,
                        prompt_summary=prompt_summary,
                        response_summary=response_summary,
                        duration_ms=duration_ms
                    )
                
                return result
            
            except Exception as e:
                logger.error(f"âŒ [{node_name}] LLM è°ƒç”¨å¤±è´¥: {e}", exc_info=True)
                raise
        
        return wrapper
    return decorator


def print_workflow_diagram():
    """æ‰“å°å·¥ä½œæµç¨‹å›¾ï¼ˆç”¨äºè°ƒè¯•å’Œæ–‡æ¡£ï¼‰"""
    diagram = """
    
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    DeepResearch LangGraph å·¥ä½œæµ                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    START
      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  parse_task     â”‚  â†’ è§£ææŸ¥è¯¢ï¼Œæå–ä»»åŠ¡è§„æ ¼
â”‚  (LLM: qwen3)   â”‚     è¾“å…¥: query
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     è¾“å‡º: task_spec, open_questions
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ build_workspace â”‚  â†’ æ„å»ºå·¥ä½œç©ºé—´ï¼ˆæ£€ç´¢ + å»é‡ï¼‰
â”‚  (Retriever)    â”‚     è¾“å…¥: query, round_id, evidence_packs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     è¾“å‡º: workspace (ç´§å‡‘å‹ï¼Œå¸¦ EvidencePack æ‘˜è¦)
         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   tool_loop     â”‚  â†’ LLM è‡ªä¸»å·¥å…·è°ƒç”¨å¾ªç¯
â”‚ (LLM + Tools)   â”‚     è¾“å…¥: workspace, open_questions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     è¾“å‡º: tool_results, new_evidence_packs, next_action
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ æ£€æŸ¥   â”‚
    â”‚action? â”‚
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”œâ”€â†’ answer/stop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                          â†“
        â””â”€â†’ continue                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â†“                             â”‚ write_answer    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚  (LLM: qwen3)   â”‚
    â”‚normalize_evidenceâ”‚ â†’ å­˜å‚¨åˆ° Milvus  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚   (Milvus)      â”‚                            â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          END
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ update_report   â”‚  â†’ æ›´æ–°ä¸­å¿ƒæŠ¥å‘Š
    â”‚  (LLM: qwen3)   â”‚     è¾“å…¥: new_evidence_packs, summary, findings
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     è¾“å‡º: updated summary, new findings, open_questions
             â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  check_stop     â”‚  â†’ æ£€æŸ¥åœæ­¢æ¡ä»¶
    â”‚  (Logic)        â”‚     - max_rounds? no_questions? low_evidence?
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ åœæ­¢?  â”‚
        â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
            â”œâ”€â†’ stop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ write_answer â†’ END
            â”‚
            â””â”€â†’ continue
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ increment_round â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
          [å¾ªç¯å› build_workspace]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

å…³é”®æ•°æ®æµï¼š
  â€¢ task_spec: ä»»åŠ¡è§„æ ¼ï¼ˆä¸»é¢˜ã€é—®é¢˜ã€è¾“å‡ºæ ¼å¼ã€çº¦æŸï¼‰
  â€¢ workspace: ç´§å‡‘å·¥ä½œç©ºé—´ï¼ˆæ‘˜è¦ + æœ€è¿‘å‘ç° + EvidencePack ç´¢å¼•ï¼‰
  â€¢ evidence_packs: æ¸è¿›å¼å±•ç¤ºçš„è¯æ®åŒ…ï¼ˆpack_id, snippet, key_pointsï¼‰
  â€¢ next_action: æµç¨‹æ§åˆ¶ï¼ˆcontinue/answer/stopï¼‰
  â€¢ findings: ç´¯ç§¯å‘ç°ï¼ˆå¸¦å¼•ç”¨ [pack_id]ï¼‰
  â€¢ open_questions: å¾…è§£å†³é—®é¢˜åˆ—è¡¨

è¿½è¸ªé…ç½®ï¼š
  â€¢ LangSmith: è®¾ç½® LANGCHAIN_API_KEY, LANGCHAIN_TRACING_V2=true
  â€¢ æœ¬åœ°è¿½è¸ª: logs/traces/trace_YYYYMMDD_HHMMSS.jsonl
    """
    print(diagram)


if __name__ == "__main__":
    # æµ‹è¯•è¿½è¸ªé…ç½®
    print_workflow_diagram()
    config = get_tracing_config()
    print(f"\nè¿½è¸ªçŠ¶æ€:")
    print(f"  LangSmith: {'âœ… å·²å¯ç”¨' if config.langsmith_enabled else 'âŒ æœªå¯ç”¨'}")
    print(f"  æœ¬åœ°è¿½è¸ª: {'âœ… å·²å¯ç”¨' if config.local_trace_enabled else 'âŒ æœªå¯ç”¨'}")
    print(f"  è¿½è¸ªæ–‡ä»¶: {config.trace_file}")
